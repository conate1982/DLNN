{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d07775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88169d7b",
   "metadata": {},
   "source": [
    "Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a408309",
   "metadata": {},
   "source": [
    "## Parameters and Hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2247fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters to handle data\n",
    "batch_size = 10\n",
    "train_valid_percentage = 0.8\n",
    "test_percentage = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fca51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparameters\n",
    "n_epochs = 400\n",
    "sgd_lr = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c1bdb",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12c2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"datos_filtrados.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb268ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaptivityLevel</th>\n",
       "      <th>Boy</th>\n",
       "      <th>Girl</th>\n",
       "      <th>1-5</th>\n",
       "      <th>11-15</th>\n",
       "      <th>16-20</th>\n",
       "      <th>21-25</th>\n",
       "      <th>26-30</th>\n",
       "      <th>6-10</th>\n",
       "      <th>College</th>\n",
       "      <th>...</th>\n",
       "      <th>NET_3G</th>\n",
       "      <th>NET_4G</th>\n",
       "      <th>Durat_0</th>\n",
       "      <th>Durat_1-3</th>\n",
       "      <th>Durat_3-6</th>\n",
       "      <th>LMS_No</th>\n",
       "      <th>LMS_Yes</th>\n",
       "      <th>Computer</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Tab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdaptivityLevel  Boy  Girl  1-5  11-15  16-20  21-25  26-30  6-10  College  \\\n",
       "0                1    1     0    0      0      0      1      0     0        0   \n",
       "1                1    0     1    0      0      0      1      0     0        0   \n",
       "2                1    0     1    0      0      1      0      0     0        1   \n",
       "3                1    0     1    0      1      0      0      0     0        0   \n",
       "4                0    0     1    0      0      1      0      0     0        0   \n",
       "\n",
       "   ...  NET_3G  NET_4G  Durat_0  Durat_1-3  Durat_3-6  LMS_No  LMS_Yes  \\\n",
       "0  ...       0       1        0          0          1       1        0   \n",
       "1  ...       0       1        0          1          0       0        1   \n",
       "2  ...       0       1        0          1          0       1        0   \n",
       "3  ...       0       1        0          1          0       1        0   \n",
       "4  ...       1       0        1          0          0       1        0   \n",
       "\n",
       "   Computer  Mobile  Tab  \n",
       "0         0       0    1  \n",
       "1         0       1    0  \n",
       "2         0       1    0  \n",
       "3         0       1    0  \n",
       "4         0       1    0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03dd3412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: AdaptivityLevel, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = datos[\"AdaptivityLevel\"]\n",
    "datos.drop(\"AdaptivityLevel\",inplace=True,axis=1)\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8b1b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = target.unique()\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d92ef",
   "metadata": {},
   "source": [
    "## Separation of training, validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61351ace",
   "metadata": {},
   "source": [
    "Se crea un función para llevar los datos desde Dataframe hacia Dataloders de Torch, pasando por tensores y Dataset de Tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41fafe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLoader(data,target):\n",
    "    dataV = data.to_numpy() #convertir fila de Dataframe Datos a arreglo\n",
    "    targetV = target.to_numpy() #convertir fila de Dataframe Target a arreglo\n",
    "    #targetV = np.reshape(target.to_numpy(),(-1,1)) #cambiar arreglo a matrix nx1\n",
    "\n",
    "    dataV = torch.from_numpy(dataV).type(torch.FloatTensor)\n",
    "    targetV = torch.from_numpy(targetV)#.type(torch.FloatTensor)\n",
    "\n",
    "    tensords = torch.utils.data.TensorDataset(dataV,targetV)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(tensords,batch_size=batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4e074",
   "metadata": {},
   "source": [
    "### TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e43f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datos.sample(frac=test_percentage, axis = 0, random_state = 3)\n",
    "test_idx = test_data.index.values\n",
    "test_target = target[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f4c271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Boy</th>\n",
       "      <th>Girl</th>\n",
       "      <th>1-5</th>\n",
       "      <th>11-15</th>\n",
       "      <th>16-20</th>\n",
       "      <th>21-25</th>\n",
       "      <th>26-30</th>\n",
       "      <th>6-10</th>\n",
       "      <th>College</th>\n",
       "      <th>School</th>\n",
       "      <th>...</th>\n",
       "      <th>NET_3G</th>\n",
       "      <th>NET_4G</th>\n",
       "      <th>Durat_0</th>\n",
       "      <th>Durat_1-3</th>\n",
       "      <th>Durat_3-6</th>\n",
       "      <th>LMS_No</th>\n",
       "      <th>LMS_Yes</th>\n",
       "      <th>Computer</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Tab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Boy  Girl  1-5  11-15  16-20  21-25  26-30  6-10  College  School  ...  \\\n",
       "822    1     0    0      1      0      0      0     0        0       1  ...   \n",
       "448    1     0    0      0      0      1      0     0        0       0  ...   \n",
       "806    0     1    0      1      0      0      0     0        0       1  ...   \n",
       "415    1     0    0      0      0      0      1     0        0       0  ...   \n",
       "290    1     0    0      1      0      0      0     0        0       1  ...   \n",
       "\n",
       "     NET_3G  NET_4G  Durat_0  Durat_1-3  Durat_3-6  LMS_No  LMS_Yes  Computer  \\\n",
       "822       0       1        0          1          0       1        0         0   \n",
       "448       0       1        0          0          1       1        0         0   \n",
       "806       0       1        0          1          0       1        0         0   \n",
       "415       0       1        1          0          0       1        0         1   \n",
       "290       0       1        0          1          0       1        0         0   \n",
       "\n",
       "     Mobile  Tab  \n",
       "822       1    0  \n",
       "448       1    0  \n",
       "806       1    0  \n",
       "415       0    0  \n",
       "290       1    0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a07ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822    2\n",
       "448    0\n",
       "806    1\n",
       "415    0\n",
       "290    1\n",
       "Name: AdaptivityLevel, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45dfc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = createLoader(test_data,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785331ca",
   "metadata": {},
   "source": [
    "### VALID DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89bde4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datos.drop(test_idx,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9acf2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = train_data.sample(frac=1-train_valid_percentage, axis = 0, random_state = 3)\n",
    "valid_idx = valid_data.index.values\n",
    "valid_target = target[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140c5ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Boy</th>\n",
       "      <th>Girl</th>\n",
       "      <th>1-5</th>\n",
       "      <th>11-15</th>\n",
       "      <th>16-20</th>\n",
       "      <th>21-25</th>\n",
       "      <th>26-30</th>\n",
       "      <th>6-10</th>\n",
       "      <th>College</th>\n",
       "      <th>School</th>\n",
       "      <th>...</th>\n",
       "      <th>NET_3G</th>\n",
       "      <th>NET_4G</th>\n",
       "      <th>Durat_0</th>\n",
       "      <th>Durat_1-3</th>\n",
       "      <th>Durat_3-6</th>\n",
       "      <th>LMS_No</th>\n",
       "      <th>LMS_Yes</th>\n",
       "      <th>Computer</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Tab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Boy  Girl  1-5  11-15  16-20  21-25  26-30  6-10  College  School  ...  \\\n",
       "789    0     1    0      0      0      1      0     0        0       0  ...   \n",
       "699    0     1    0      0      1      0      0     0        1       0  ...   \n",
       "693    0     1    1      0      0      0      0     0        0       1  ...   \n",
       "372    0     1    0      0      1      0      0     0        0       0  ...   \n",
       "267    0     1    0      0      1      0      0     0        0       0  ...   \n",
       "\n",
       "     NET_3G  NET_4G  Durat_0  Durat_1-3  Durat_3-6  LMS_No  LMS_Yes  Computer  \\\n",
       "789       0       1        0          0          1       1        0         1   \n",
       "699       0       1        0          1          0       1        0         0   \n",
       "693       0       1        0          1          0       1        0         0   \n",
       "372       0       1        0          1          0       0        1         0   \n",
       "267       0       1        0          1          0       0        1         0   \n",
       "\n",
       "     Mobile  Tab  \n",
       "789       0    0  \n",
       "699       1    0  \n",
       "693       1    0  \n",
       "372       1    0  \n",
       "267       1    0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9bfc9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789    1\n",
       "699    0\n",
       "693    1\n",
       "372    0\n",
       "267    0\n",
       "Name: AdaptivityLevel, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a4ecd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = createLoader(test_data,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15281447",
   "metadata": {},
   "source": [
    "### TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e291e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(valid_idx, axis=0)\n",
    "train_idx = train_data.index.values\n",
    "train_target = target[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9ac1d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Boy</th>\n",
       "      <th>Girl</th>\n",
       "      <th>1-5</th>\n",
       "      <th>11-15</th>\n",
       "      <th>16-20</th>\n",
       "      <th>21-25</th>\n",
       "      <th>26-30</th>\n",
       "      <th>6-10</th>\n",
       "      <th>College</th>\n",
       "      <th>School</th>\n",
       "      <th>...</th>\n",
       "      <th>NET_3G</th>\n",
       "      <th>NET_4G</th>\n",
       "      <th>Durat_0</th>\n",
       "      <th>Durat_1-3</th>\n",
       "      <th>Durat_3-6</th>\n",
       "      <th>LMS_No</th>\n",
       "      <th>LMS_Yes</th>\n",
       "      <th>Computer</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Tab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Boy  Girl  1-5  11-15  16-20  21-25  26-30  6-10  College  School  ...  \\\n",
       "0     1     0    0      0      0      1      0     0        0       0  ...   \n",
       "2     0     1    0      0      1      0      0     0        1       0  ...   \n",
       "5     1     0    0      1      0      0      0     0        0       1  ...   \n",
       "9     1     0    0      1      0      0      0     0        0       1  ...   \n",
       "10    0     1    0      0      1      0      0     0        0       0  ...   \n",
       "\n",
       "    NET_3G  NET_4G  Durat_0  Durat_1-3  Durat_3-6  LMS_No  LMS_Yes  Computer  \\\n",
       "0        0       1        0          0          1       1        0         0   \n",
       "2        0       1        0          1          0       1        0         0   \n",
       "5        1       0        0          1          0       1        0         0   \n",
       "9        1       0        0          1          0       1        0         0   \n",
       "10       0       1        0          1          0       1        0         0   \n",
       "\n",
       "    Mobile  Tab  \n",
       "0        0    1  \n",
       "2        1    0  \n",
       "5        1    0  \n",
       "9        1    0  \n",
       "10       1    0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9698f757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "2     1\n",
       "5     0\n",
       "9     1\n",
       "10    0\n",
       "Name: AdaptivityLevel, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8be7ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = createLoader(test_data,test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832b162f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData shape :  (768, 35)\n",
      "Train Loader Len:  24\n"
     ]
    }
   ],
   "source": [
    "print(\"TrainData shape : \",train_data.shape)\n",
    "print(\"Train Loader Len: \",len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a98c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Data shape :  (192, 35)\n",
      "Valid Loader Len:  24\n"
     ]
    }
   ],
   "source": [
    "print(\"Valid Data shape : \",valid_data.shape)\n",
    "print(\"Valid Loader Len: \",len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e999a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data shape :  (240, 35)\n",
      "Test Loader Len:  24\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Data shape : \",test_data.shape)\n",
    "print(\"Test Loader Len: \",len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77a24f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a5649d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 35])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dato,resp = dataiter.next()\n",
    "print(dato.shape)\n",
    "print(resp.shape)\n",
    "dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e59174fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1, 0, 1, 2, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6844a9",
   "metadata": {},
   "source": [
    "## Designing the Neural Network Architecture.\n",
    "## Define the model´s hyper-parameters.\n",
    "## Train and validate the model from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eedbd140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "train_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93c6b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a68813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModNN(\n",
      "  (fc1): Linear(in_features=35, out_features=18, bias=True)\n",
      "  (fc2): Linear(in_features=18, out_features=9, bias=True)\n",
      "  (fc3): Linear(in_features=9, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the CNN architecture\n",
    "class ModNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes): \n",
    "    #def __init__(self, input_size,num_classes): \n",
    "        #input size = 1 fila x 36 features\n",
    "        #num_classes = 3 -> low, medium, high\n",
    "        super(ModNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 18) # input_size(35) -> 18\n",
    "        self.fc2 = nn.Linear(18, 9) # 18 -> 9\n",
    "        self.fc3 = nn.Linear(9, num_classes)  # 9 -> num_classes (3)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"MODNN: x1: \",x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "#Entran 35 features, sale 1 clasificación\n",
    "model = ModNN(35,len(classes))\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d2cb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=sgd_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07fb19ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 9.949332 \tValidation Loss: 9.333250\n",
      "Validation loss decreased (inf --> 9.333250).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 9.224183 \tValidation Loss: 8.874609\n",
      "Validation loss decreased (9.333250 --> 8.874609).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 8.839066 \tValidation Loss: 8.700475\n",
      "Validation loss decreased (8.874609 --> 8.700475).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 8.922913 \tValidation Loss: 8.648073\n",
      "Validation loss decreased (8.700475 --> 8.648073).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 8.698206 \tValidation Loss: 8.494921\n",
      "Validation loss decreased (8.648073 --> 8.494921).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 8.563344 \tValidation Loss: 8.428954\n",
      "Validation loss decreased (8.494921 --> 8.428954).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 8.492707 \tValidation Loss: 8.275823\n",
      "Validation loss decreased (8.428954 --> 8.275823).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 8.587778 \tValidation Loss: 8.234981\n",
      "Validation loss decreased (8.275823 --> 8.234981).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 8.365459 \tValidation Loss: 8.052523\n",
      "Validation loss decreased (8.234981 --> 8.052523).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 8.123145 \tValidation Loss: 7.983987\n",
      "Validation loss decreased (8.052523 --> 7.983987).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 8.260748 \tValidation Loss: 7.955473\n",
      "Validation loss decreased (7.983987 --> 7.955473).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 8.131389 \tValidation Loss: 7.825627\n",
      "Validation loss decreased (7.955473 --> 7.825627).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 8.085134 \tValidation Loss: 7.766831\n",
      "Validation loss decreased (7.825627 --> 7.766831).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 7.936118 \tValidation Loss: 7.589446\n",
      "Validation loss decreased (7.766831 --> 7.589446).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 7.554886 \tValidation Loss: 7.339917\n",
      "Validation loss decreased (7.589446 --> 7.339917).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 8.174491 \tValidation Loss: 7.492420\n",
      "Epoch: 17 \tTraining Loss: 7.538727 \tValidation Loss: 7.309282\n",
      "Validation loss decreased (7.339917 --> 7.309282).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 7.565991 \tValidation Loss: 7.120997\n",
      "Validation loss decreased (7.309282 --> 7.120997).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 7.641933 \tValidation Loss: 7.089277\n",
      "Validation loss decreased (7.120997 --> 7.089277).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 7.492895 \tValidation Loss: 7.234223\n",
      "Epoch: 21 \tTraining Loss: 7.400734 \tValidation Loss: 7.048305\n",
      "Validation loss decreased (7.089277 --> 7.048305).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 7.429426 \tValidation Loss: 7.051683\n",
      "Epoch: 23 \tTraining Loss: 7.154946 \tValidation Loss: 6.746709\n",
      "Validation loss decreased (7.048305 --> 6.746709).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 7.404951 \tValidation Loss: 7.137798\n",
      "Epoch: 25 \tTraining Loss: 7.206666 \tValidation Loss: 6.875212\n",
      "Epoch: 26 \tTraining Loss: 7.221158 \tValidation Loss: 6.609295\n",
      "Validation loss decreased (6.746709 --> 6.609295).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 6.919874 \tValidation Loss: 6.834080\n",
      "Epoch: 28 \tTraining Loss: 6.904427 \tValidation Loss: 6.504285\n",
      "Validation loss decreased (6.609295 --> 6.504285).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 6.887600 \tValidation Loss: 6.676082\n",
      "Epoch: 30 \tTraining Loss: 6.605289 \tValidation Loss: 6.546340\n",
      "Epoch: 31 \tTraining Loss: 6.368942 \tValidation Loss: 6.385132\n",
      "Validation loss decreased (6.504285 --> 6.385132).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 6.791651 \tValidation Loss: 6.484579\n",
      "Epoch: 33 \tTraining Loss: 6.046346 \tValidation Loss: 6.201692\n",
      "Validation loss decreased (6.385132 --> 6.201692).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 6.320057 \tValidation Loss: 6.268230\n",
      "Epoch: 35 \tTraining Loss: 6.294462 \tValidation Loss: 5.936170\n",
      "Validation loss decreased (6.201692 --> 5.936170).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 6.583473 \tValidation Loss: 5.978082\n",
      "Epoch: 37 \tTraining Loss: 5.955190 \tValidation Loss: 5.730458\n",
      "Validation loss decreased (5.936170 --> 5.730458).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 6.316441 \tValidation Loss: 5.505301\n",
      "Validation loss decreased (5.730458 --> 5.505301).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 6.021029 \tValidation Loss: 5.898493\n",
      "Epoch: 40 \tTraining Loss: 6.217481 \tValidation Loss: 5.825626\n",
      "Epoch: 41 \tTraining Loss: 5.815582 \tValidation Loss: 5.271218\n",
      "Validation loss decreased (5.505301 --> 5.271218).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 6.225362 \tValidation Loss: 5.431609\n",
      "Epoch: 43 \tTraining Loss: 5.606185 \tValidation Loss: 5.050244\n",
      "Validation loss decreased (5.271218 --> 5.050244).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 5.762084 \tValidation Loss: 5.050819\n",
      "Epoch: 45 \tTraining Loss: 5.780114 \tValidation Loss: 5.321141\n",
      "Epoch: 46 \tTraining Loss: 5.894979 \tValidation Loss: 4.835354\n",
      "Validation loss decreased (5.050244 --> 4.835354).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 5.941293 \tValidation Loss: 5.299744\n",
      "Epoch: 48 \tTraining Loss: 5.978649 \tValidation Loss: 4.924238\n",
      "Epoch: 49 \tTraining Loss: 5.430650 \tValidation Loss: 5.315021\n",
      "Epoch: 50 \tTraining Loss: 5.150437 \tValidation Loss: 4.820189\n",
      "Validation loss decreased (4.835354 --> 4.820189).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 5.229105 \tValidation Loss: 4.615957\n",
      "Validation loss decreased (4.820189 --> 4.615957).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 5.709603 \tValidation Loss: 5.466728\n",
      "Epoch: 53 \tTraining Loss: 5.300066 \tValidation Loss: 4.856400\n",
      "Epoch: 54 \tTraining Loss: 5.554482 \tValidation Loss: 4.617594\n",
      "Epoch: 55 \tTraining Loss: 5.241593 \tValidation Loss: 4.796732\n",
      "Epoch: 56 \tTraining Loss: 5.363868 \tValidation Loss: 4.596413\n",
      "Validation loss decreased (4.615957 --> 4.596413).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 5.506779 \tValidation Loss: 4.708639\n",
      "Epoch: 58 \tTraining Loss: 5.094670 \tValidation Loss: 4.303425\n",
      "Validation loss decreased (4.596413 --> 4.303425).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 4.974410 \tValidation Loss: 4.358885\n",
      "Epoch: 60 \tTraining Loss: 5.216684 \tValidation Loss: 4.540780\n",
      "Epoch: 61 \tTraining Loss: 4.931903 \tValidation Loss: 4.333359\n",
      "Epoch: 62 \tTraining Loss: 5.133665 \tValidation Loss: 4.177822\n",
      "Validation loss decreased (4.303425 --> 4.177822).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 5.117573 \tValidation Loss: 4.974040\n",
      "Epoch: 64 \tTraining Loss: 5.070889 \tValidation Loss: 4.547900\n",
      "Epoch: 65 \tTraining Loss: 5.084161 \tValidation Loss: 4.185333\n",
      "Epoch: 66 \tTraining Loss: 5.419817 \tValidation Loss: 4.395896\n",
      "Epoch: 67 \tTraining Loss: 4.806866 \tValidation Loss: 3.851950\n",
      "Validation loss decreased (4.177822 --> 3.851950).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 4.883929 \tValidation Loss: 4.154911\n",
      "Epoch: 69 \tTraining Loss: 4.718733 \tValidation Loss: 4.216953\n",
      "Epoch: 70 \tTraining Loss: 4.830992 \tValidation Loss: 4.255546\n",
      "Epoch: 71 \tTraining Loss: 4.750705 \tValidation Loss: 4.039238\n",
      "Epoch: 72 \tTraining Loss: 4.411160 \tValidation Loss: 4.224815\n",
      "Epoch: 73 \tTraining Loss: 5.170065 \tValidation Loss: 3.995550\n",
      "Epoch: 74 \tTraining Loss: 5.013159 \tValidation Loss: 4.227395\n",
      "Epoch: 75 \tTraining Loss: 4.968062 \tValidation Loss: 3.696457\n",
      "Validation loss decreased (3.851950 --> 3.696457).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 4.514490 \tValidation Loss: 4.030754\n",
      "Epoch: 77 \tTraining Loss: 4.622261 \tValidation Loss: 4.104206\n",
      "Epoch: 78 \tTraining Loss: 4.557309 \tValidation Loss: 4.012784\n",
      "Epoch: 79 \tTraining Loss: 5.036431 \tValidation Loss: 3.546533\n",
      "Validation loss decreased (3.696457 --> 3.546533).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 4.334478 \tValidation Loss: 3.553572\n",
      "Epoch: 81 \tTraining Loss: 4.684858 \tValidation Loss: 3.928589\n",
      "Epoch: 82 \tTraining Loss: 4.176182 \tValidation Loss: 3.441587\n",
      "Validation loss decreased (3.546533 --> 3.441587).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 4.203347 \tValidation Loss: 3.520719\n",
      "Epoch: 84 \tTraining Loss: 4.633329 \tValidation Loss: 3.488179\n",
      "Epoch: 85 \tTraining Loss: 4.436354 \tValidation Loss: 3.400259\n",
      "Validation loss decreased (3.441587 --> 3.400259).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 4.412014 \tValidation Loss: 3.155671\n",
      "Validation loss decreased (3.400259 --> 3.155671).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 4.386482 \tValidation Loss: 3.773553\n",
      "Epoch: 88 \tTraining Loss: 4.211455 \tValidation Loss: 3.098996\n",
      "Validation loss decreased (3.155671 --> 3.098996).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 \tTraining Loss: 4.117433 \tValidation Loss: 3.366521\n",
      "Epoch: 90 \tTraining Loss: 4.211393 \tValidation Loss: 3.336284\n",
      "Epoch: 91 \tTraining Loss: 3.947673 \tValidation Loss: 3.097281\n",
      "Validation loss decreased (3.098996 --> 3.097281).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 4.222687 \tValidation Loss: 3.394961\n",
      "Epoch: 93 \tTraining Loss: 3.993221 \tValidation Loss: 3.070246\n",
      "Validation loss decreased (3.097281 --> 3.070246).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 3.908855 \tValidation Loss: 3.596929\n",
      "Epoch: 95 \tTraining Loss: 3.369203 \tValidation Loss: 3.187084\n",
      "Epoch: 96 \tTraining Loss: 4.123273 \tValidation Loss: 2.997627\n",
      "Validation loss decreased (3.070246 --> 2.997627).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 4.029681 \tValidation Loss: 3.212087\n",
      "Epoch: 98 \tTraining Loss: 4.025513 \tValidation Loss: 3.357329\n",
      "Epoch: 99 \tTraining Loss: 4.122216 \tValidation Loss: 3.296643\n",
      "Epoch: 100 \tTraining Loss: 3.918818 \tValidation Loss: 3.249068\n",
      "Epoch: 101 \tTraining Loss: 4.334431 \tValidation Loss: 3.184324\n",
      "Epoch: 102 \tTraining Loss: 3.503402 \tValidation Loss: 3.213759\n",
      "Epoch: 103 \tTraining Loss: 3.630478 \tValidation Loss: 3.190638\n",
      "Epoch: 104 \tTraining Loss: 3.772783 \tValidation Loss: 3.110890\n",
      "Epoch: 105 \tTraining Loss: 4.439283 \tValidation Loss: 3.649071\n",
      "Epoch: 106 \tTraining Loss: 3.782336 \tValidation Loss: 2.957211\n",
      "Validation loss decreased (2.997627 --> 2.957211).  Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 3.619012 \tValidation Loss: 2.765257\n",
      "Validation loss decreased (2.957211 --> 2.765257).  Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 3.778635 \tValidation Loss: 2.852909\n",
      "Epoch: 109 \tTraining Loss: 3.685887 \tValidation Loss: 2.826855\n",
      "Epoch: 110 \tTraining Loss: 3.709344 \tValidation Loss: 3.113268\n",
      "Epoch: 111 \tTraining Loss: 3.653411 \tValidation Loss: 3.068738\n",
      "Epoch: 112 \tTraining Loss: 4.228217 \tValidation Loss: 2.708076\n",
      "Validation loss decreased (2.765257 --> 2.708076).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 3.896927 \tValidation Loss: 2.605834\n",
      "Validation loss decreased (2.708076 --> 2.605834).  Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 3.241442 \tValidation Loss: 2.586121\n",
      "Validation loss decreased (2.605834 --> 2.586121).  Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 3.658355 \tValidation Loss: 3.064446\n",
      "Epoch: 116 \tTraining Loss: 3.653713 \tValidation Loss: 2.748811\n",
      "Epoch: 117 \tTraining Loss: 3.479873 \tValidation Loss: 3.461195\n",
      "Epoch: 118 \tTraining Loss: 3.890463 \tValidation Loss: 3.099822\n",
      "Epoch: 119 \tTraining Loss: 3.451095 \tValidation Loss: 3.088497\n",
      "Epoch: 120 \tTraining Loss: 3.970864 \tValidation Loss: 2.503868\n",
      "Validation loss decreased (2.586121 --> 2.503868).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 3.427263 \tValidation Loss: 2.899958\n",
      "Epoch: 122 \tTraining Loss: 3.670908 \tValidation Loss: 2.407749\n",
      "Validation loss decreased (2.503868 --> 2.407749).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 3.925966 \tValidation Loss: 2.962494\n",
      "Epoch: 124 \tTraining Loss: 3.674400 \tValidation Loss: 2.942333\n",
      "Epoch: 125 \tTraining Loss: 3.422063 \tValidation Loss: 2.561798\n",
      "Epoch: 126 \tTraining Loss: 3.714858 \tValidation Loss: 2.523098\n",
      "Epoch: 127 \tTraining Loss: 3.234612 \tValidation Loss: 2.760785\n",
      "Epoch: 128 \tTraining Loss: 3.743139 \tValidation Loss: 2.475873\n",
      "Epoch: 129 \tTraining Loss: 3.397357 \tValidation Loss: 2.414762\n",
      "Epoch: 130 \tTraining Loss: 3.788190 \tValidation Loss: 2.455638\n",
      "Epoch: 131 \tTraining Loss: 3.422506 \tValidation Loss: 2.562196\n",
      "Epoch: 132 \tTraining Loss: 3.640244 \tValidation Loss: 2.323987\n",
      "Validation loss decreased (2.407749 --> 2.323987).  Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 3.570565 \tValidation Loss: 2.330595\n",
      "Epoch: 134 \tTraining Loss: 3.280943 \tValidation Loss: 2.289213\n",
      "Validation loss decreased (2.323987 --> 2.289213).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 3.342692 \tValidation Loss: 3.055613\n",
      "Epoch: 136 \tTraining Loss: 3.754206 \tValidation Loss: 2.634218\n",
      "Epoch: 137 \tTraining Loss: 3.877863 \tValidation Loss: 2.991801\n",
      "Epoch: 138 \tTraining Loss: 3.678263 \tValidation Loss: 2.980532\n",
      "Epoch: 139 \tTraining Loss: 3.695523 \tValidation Loss: 2.494024\n",
      "Epoch: 140 \tTraining Loss: 3.096399 \tValidation Loss: 2.419467\n",
      "Epoch: 141 \tTraining Loss: 3.273773 \tValidation Loss: 2.591892\n",
      "Epoch: 142 \tTraining Loss: 2.796534 \tValidation Loss: 2.545058\n",
      "Epoch: 143 \tTraining Loss: 3.356239 \tValidation Loss: 2.129815\n",
      "Validation loss decreased (2.289213 --> 2.129815).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 3.215800 \tValidation Loss: 2.121109\n",
      "Validation loss decreased (2.129815 --> 2.121109).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 3.132197 \tValidation Loss: 2.286349\n",
      "Epoch: 146 \tTraining Loss: 3.056590 \tValidation Loss: 2.372426\n",
      "Epoch: 147 \tTraining Loss: 3.202143 \tValidation Loss: 2.054342\n",
      "Validation loss decreased (2.121109 --> 2.054342).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 3.373877 \tValidation Loss: 2.711321\n",
      "Epoch: 149 \tTraining Loss: 3.090138 \tValidation Loss: 2.590739\n",
      "Epoch: 150 \tTraining Loss: 3.501739 \tValidation Loss: 2.314209\n",
      "Epoch: 151 \tTraining Loss: 3.353930 \tValidation Loss: 2.469753\n",
      "Epoch: 152 \tTraining Loss: 3.268421 \tValidation Loss: 2.064026\n",
      "Epoch: 153 \tTraining Loss: 2.866935 \tValidation Loss: 2.394320\n",
      "Epoch: 154 \tTraining Loss: 3.276922 \tValidation Loss: 1.994333\n",
      "Validation loss decreased (2.054342 --> 1.994333).  Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 3.414668 \tValidation Loss: 2.584637\n",
      "Epoch: 156 \tTraining Loss: 2.973570 \tValidation Loss: 2.271593\n",
      "Epoch: 157 \tTraining Loss: 3.380283 \tValidation Loss: 2.460223\n",
      "Epoch: 158 \tTraining Loss: 3.090249 \tValidation Loss: 1.848130\n",
      "Validation loss decreased (1.994333 --> 1.848130).  Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 3.060470 \tValidation Loss: 2.184313\n",
      "Epoch: 160 \tTraining Loss: 2.910449 \tValidation Loss: 1.895516\n",
      "Epoch: 161 \tTraining Loss: 3.194481 \tValidation Loss: 2.038141\n",
      "Epoch: 162 \tTraining Loss: 2.923304 \tValidation Loss: 1.932780\n",
      "Epoch: 163 \tTraining Loss: 3.073207 \tValidation Loss: 1.943411\n",
      "Epoch: 164 \tTraining Loss: 3.068482 \tValidation Loss: 2.022009\n",
      "Epoch: 165 \tTraining Loss: 3.261744 \tValidation Loss: 2.501469\n",
      "Epoch: 166 \tTraining Loss: 3.454568 \tValidation Loss: 1.976273\n",
      "Epoch: 167 \tTraining Loss: 2.981588 \tValidation Loss: 2.341614\n",
      "Epoch: 168 \tTraining Loss: 3.126910 \tValidation Loss: 3.282180\n",
      "Epoch: 169 \tTraining Loss: 2.796981 \tValidation Loss: 2.010062\n",
      "Epoch: 170 \tTraining Loss: 3.520175 \tValidation Loss: 2.198557\n",
      "Epoch: 171 \tTraining Loss: 3.472932 \tValidation Loss: 2.488325\n",
      "Epoch: 172 \tTraining Loss: 3.304995 \tValidation Loss: 3.178782\n",
      "Epoch: 173 \tTraining Loss: 3.373487 \tValidation Loss: 1.965443\n",
      "Epoch: 174 \tTraining Loss: 2.804751 \tValidation Loss: 2.613943\n",
      "Epoch: 175 \tTraining Loss: 3.034924 \tValidation Loss: 1.822675\n",
      "Validation loss decreased (1.848130 --> 1.822675).  Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 2.837028 \tValidation Loss: 2.138930\n",
      "Epoch: 177 \tTraining Loss: 3.227610 \tValidation Loss: 2.504913\n",
      "Epoch: 178 \tTraining Loss: 3.608597 \tValidation Loss: 2.041578\n",
      "Epoch: 179 \tTraining Loss: 3.280472 \tValidation Loss: 2.489328\n",
      "Epoch: 180 \tTraining Loss: 3.097552 \tValidation Loss: 2.316675\n",
      "Epoch: 181 \tTraining Loss: 3.389957 \tValidation Loss: 2.196335\n",
      "Epoch: 182 \tTraining Loss: 3.727046 \tValidation Loss: 2.073395\n",
      "Epoch: 183 \tTraining Loss: 3.102855 \tValidation Loss: 2.216540\n",
      "Epoch: 184 \tTraining Loss: 3.043853 \tValidation Loss: 1.875405\n",
      "Epoch: 185 \tTraining Loss: 3.386448 \tValidation Loss: 1.909716\n",
      "Epoch: 186 \tTraining Loss: 2.764861 \tValidation Loss: 1.764017\n",
      "Validation loss decreased (1.822675 --> 1.764017).  Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 2.681034 \tValidation Loss: 1.847689\n",
      "Epoch: 188 \tTraining Loss: 2.744914 \tValidation Loss: 1.601782\n",
      "Validation loss decreased (1.764017 --> 1.601782).  Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 2.953026 \tValidation Loss: 2.175719\n",
      "Epoch: 190 \tTraining Loss: 2.977234 \tValidation Loss: 1.821410\n",
      "Epoch: 191 \tTraining Loss: 3.303215 \tValidation Loss: 1.985917\n",
      "Epoch: 192 \tTraining Loss: 2.802922 \tValidation Loss: 1.876538\n",
      "Epoch: 193 \tTraining Loss: 3.177044 \tValidation Loss: 2.000807\n",
      "Epoch: 194 \tTraining Loss: 3.298273 \tValidation Loss: 2.053386\n",
      "Epoch: 195 \tTraining Loss: 2.918172 \tValidation Loss: 1.858279\n",
      "Epoch: 196 \tTraining Loss: 2.856957 \tValidation Loss: 2.172475\n",
      "Epoch: 197 \tTraining Loss: 3.045214 \tValidation Loss: 1.951705\n",
      "Epoch: 198 \tTraining Loss: 2.970076 \tValidation Loss: 2.011602\n",
      "Epoch: 199 \tTraining Loss: 2.727881 \tValidation Loss: 1.773692\n",
      "Epoch: 200 \tTraining Loss: 2.631983 \tValidation Loss: 1.767454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 201 \tTraining Loss: 3.103329 \tValidation Loss: 2.016477\n",
      "Epoch: 202 \tTraining Loss: 3.158539 \tValidation Loss: 2.063827\n",
      "Epoch: 203 \tTraining Loss: 3.574639 \tValidation Loss: 2.070706\n",
      "Epoch: 204 \tTraining Loss: 2.949502 \tValidation Loss: 1.682104\n",
      "Epoch: 205 \tTraining Loss: 2.978199 \tValidation Loss: 1.911748\n",
      "Epoch: 206 \tTraining Loss: 3.251203 \tValidation Loss: 2.028141\n",
      "Epoch: 207 \tTraining Loss: 2.733437 \tValidation Loss: 1.766763\n",
      "Epoch: 208 \tTraining Loss: 3.022737 \tValidation Loss: 2.114015\n",
      "Epoch: 209 \tTraining Loss: 3.025887 \tValidation Loss: 1.858362\n",
      "Epoch: 210 \tTraining Loss: 2.923392 \tValidation Loss: 1.886671\n",
      "Epoch: 211 \tTraining Loss: 2.863178 \tValidation Loss: 1.726691\n",
      "Epoch: 212 \tTraining Loss: 2.804945 \tValidation Loss: 1.989017\n",
      "Epoch: 213 \tTraining Loss: 3.183354 \tValidation Loss: 1.908505\n",
      "Epoch: 214 \tTraining Loss: 2.893107 \tValidation Loss: 1.683257\n",
      "Epoch: 215 \tTraining Loss: 2.930309 \tValidation Loss: 1.612949\n",
      "Epoch: 216 \tTraining Loss: 2.959806 \tValidation Loss: 1.918861\n",
      "Epoch: 217 \tTraining Loss: 2.968871 \tValidation Loss: 2.040307\n",
      "Epoch: 218 \tTraining Loss: 3.048969 \tValidation Loss: 1.806265\n",
      "Epoch: 219 \tTraining Loss: 2.727144 \tValidation Loss: 1.923357\n",
      "Epoch: 220 \tTraining Loss: 2.707164 \tValidation Loss: 2.233775\n",
      "Epoch: 221 \tTraining Loss: 3.076130 \tValidation Loss: 1.715054\n",
      "Epoch: 222 \tTraining Loss: 3.264090 \tValidation Loss: 2.170605\n",
      "Epoch: 223 \tTraining Loss: 2.824119 \tValidation Loss: 1.723519\n",
      "Epoch: 224 \tTraining Loss: 3.041086 \tValidation Loss: 1.812428\n",
      "Epoch: 225 \tTraining Loss: 3.007897 \tValidation Loss: 1.886573\n",
      "Epoch: 226 \tTraining Loss: 2.689947 \tValidation Loss: 1.697860\n",
      "Epoch: 227 \tTraining Loss: 2.698427 \tValidation Loss: 1.842614\n",
      "Epoch: 228 \tTraining Loss: 2.667496 \tValidation Loss: 1.745757\n",
      "Epoch: 229 \tTraining Loss: 2.753114 \tValidation Loss: 1.754893\n",
      "Epoch: 230 \tTraining Loss: 3.435113 \tValidation Loss: 1.940769\n",
      "Epoch: 231 \tTraining Loss: 3.076580 \tValidation Loss: 1.861214\n",
      "Epoch: 232 \tTraining Loss: 2.883702 \tValidation Loss: 1.772202\n",
      "Epoch: 233 \tTraining Loss: 2.792477 \tValidation Loss: 1.648890\n",
      "Epoch: 234 \tTraining Loss: 2.361411 \tValidation Loss: 1.682790\n",
      "Epoch: 235 \tTraining Loss: 2.471097 \tValidation Loss: 1.591903\n",
      "Validation loss decreased (1.601782 --> 1.591903).  Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 3.014530 \tValidation Loss: 1.616194\n",
      "Epoch: 237 \tTraining Loss: 3.297911 \tValidation Loss: 1.895722\n",
      "Epoch: 238 \tTraining Loss: 2.892230 \tValidation Loss: 1.774068\n",
      "Epoch: 239 \tTraining Loss: 3.056363 \tValidation Loss: 2.066054\n",
      "Epoch: 240 \tTraining Loss: 2.526578 \tValidation Loss: 1.738233\n",
      "Epoch: 241 \tTraining Loss: 2.928946 \tValidation Loss: 1.723081\n",
      "Epoch: 242 \tTraining Loss: 2.761876 \tValidation Loss: 1.945489\n",
      "Epoch: 243 \tTraining Loss: 2.923060 \tValidation Loss: 1.806839\n",
      "Epoch: 244 \tTraining Loss: 2.912957 \tValidation Loss: 1.930778\n",
      "Epoch: 245 \tTraining Loss: 2.813931 \tValidation Loss: 1.741647\n",
      "Epoch: 246 \tTraining Loss: 2.981721 \tValidation Loss: 1.740428\n",
      "Epoch: 247 \tTraining Loss: 3.332996 \tValidation Loss: 1.769644\n",
      "Epoch: 248 \tTraining Loss: 3.015533 \tValidation Loss: 1.736085\n",
      "Epoch: 249 \tTraining Loss: 3.191099 \tValidation Loss: 1.745507\n",
      "Epoch: 250 \tTraining Loss: 2.585529 \tValidation Loss: 1.836560\n",
      "Epoch: 251 \tTraining Loss: 2.787507 \tValidation Loss: 1.807163\n",
      "Epoch: 252 \tTraining Loss: 2.797081 \tValidation Loss: 1.690906\n",
      "Epoch: 253 \tTraining Loss: 2.651306 \tValidation Loss: 1.751186\n",
      "Epoch: 254 \tTraining Loss: 2.859363 \tValidation Loss: 1.654534\n",
      "Epoch: 255 \tTraining Loss: 3.384152 \tValidation Loss: 1.716014\n",
      "Epoch: 256 \tTraining Loss: 2.369370 \tValidation Loss: 1.595788\n",
      "Epoch: 257 \tTraining Loss: 3.024972 \tValidation Loss: 1.763114\n",
      "Epoch: 258 \tTraining Loss: 3.046383 \tValidation Loss: 1.678335\n",
      "Epoch: 259 \tTraining Loss: 2.623874 \tValidation Loss: 1.700547\n",
      "Epoch: 260 \tTraining Loss: 2.797684 \tValidation Loss: 1.605845\n",
      "Epoch: 261 \tTraining Loss: 2.394537 \tValidation Loss: 1.605168\n",
      "Epoch: 262 \tTraining Loss: 2.539897 \tValidation Loss: 1.619018\n",
      "Epoch: 263 \tTraining Loss: 2.421214 \tValidation Loss: 1.580604\n",
      "Validation loss decreased (1.591903 --> 1.580604).  Saving model ...\n",
      "Epoch: 264 \tTraining Loss: 3.110819 \tValidation Loss: 1.911245\n",
      "Epoch: 265 \tTraining Loss: 2.983604 \tValidation Loss: 1.574713\n",
      "Validation loss decreased (1.580604 --> 1.574713).  Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 2.529218 \tValidation Loss: 1.687082\n",
      "Epoch: 267 \tTraining Loss: 2.536195 \tValidation Loss: 1.750045\n",
      "Epoch: 268 \tTraining Loss: 2.881639 \tValidation Loss: 1.628124\n",
      "Epoch: 269 \tTraining Loss: 2.750964 \tValidation Loss: 1.567670\n",
      "Validation loss decreased (1.574713 --> 1.567670).  Saving model ...\n",
      "Epoch: 270 \tTraining Loss: 2.486943 \tValidation Loss: 1.480167\n",
      "Validation loss decreased (1.567670 --> 1.480167).  Saving model ...\n",
      "Epoch: 271 \tTraining Loss: 2.578056 \tValidation Loss: 1.633295\n",
      "Epoch: 272 \tTraining Loss: 2.834238 \tValidation Loss: 1.658188\n",
      "Epoch: 273 \tTraining Loss: 3.039133 \tValidation Loss: 1.740435\n",
      "Epoch: 274 \tTraining Loss: 2.731305 \tValidation Loss: 1.703243\n",
      "Epoch: 275 \tTraining Loss: 2.619766 \tValidation Loss: 1.615535\n",
      "Epoch: 276 \tTraining Loss: 2.740681 \tValidation Loss: 1.655772\n",
      "Epoch: 277 \tTraining Loss: 2.972659 \tValidation Loss: 1.653312\n",
      "Epoch: 278 \tTraining Loss: 2.754761 \tValidation Loss: 1.664238\n",
      "Epoch: 279 \tTraining Loss: 3.130785 \tValidation Loss: 1.804996\n",
      "Epoch: 280 \tTraining Loss: 2.663053 \tValidation Loss: 1.615439\n",
      "Epoch: 281 \tTraining Loss: 3.064020 \tValidation Loss: 1.804044\n",
      "Epoch: 282 \tTraining Loss: 2.800827 \tValidation Loss: 1.595017\n",
      "Epoch: 283 \tTraining Loss: 2.483999 \tValidation Loss: 1.720015\n",
      "Epoch: 284 \tTraining Loss: 2.962171 \tValidation Loss: 1.989660\n",
      "Epoch: 285 \tTraining Loss: 2.639915 \tValidation Loss: 1.606183\n",
      "Epoch: 286 \tTraining Loss: 2.568308 \tValidation Loss: 1.546993\n",
      "Epoch: 287 \tTraining Loss: 2.942618 \tValidation Loss: 1.509708\n",
      "Epoch: 288 \tTraining Loss: 2.972148 \tValidation Loss: 1.568622\n",
      "Epoch: 289 \tTraining Loss: 2.454096 \tValidation Loss: 1.606966\n",
      "Epoch: 290 \tTraining Loss: 2.956834 \tValidation Loss: 1.545585\n",
      "Epoch: 291 \tTraining Loss: 2.835549 \tValidation Loss: 1.653135\n",
      "Epoch: 292 \tTraining Loss: 2.278082 \tValidation Loss: 1.492492\n",
      "Epoch: 293 \tTraining Loss: 2.979266 \tValidation Loss: 1.647245\n",
      "Epoch: 294 \tTraining Loss: 2.566125 \tValidation Loss: 1.661396\n",
      "Epoch: 295 \tTraining Loss: 2.651205 \tValidation Loss: 1.523887\n",
      "Epoch: 296 \tTraining Loss: 2.765516 \tValidation Loss: 1.621670\n",
      "Epoch: 297 \tTraining Loss: 2.565084 \tValidation Loss: 1.609245\n",
      "Epoch: 298 \tTraining Loss: 2.422982 \tValidation Loss: 1.557956\n",
      "Epoch: 299 \tTraining Loss: 2.332237 \tValidation Loss: 1.705125\n",
      "Epoch: 300 \tTraining Loss: 2.685504 \tValidation Loss: 1.505982\n",
      "Epoch: 301 \tTraining Loss: 2.752764 \tValidation Loss: 1.674981\n",
      "Epoch: 302 \tTraining Loss: 3.008230 \tValidation Loss: 1.544774\n",
      "Epoch: 303 \tTraining Loss: 2.954994 \tValidation Loss: 1.695199\n",
      "Epoch: 304 \tTraining Loss: 2.521781 \tValidation Loss: 1.488691\n",
      "Epoch: 305 \tTraining Loss: 2.487101 \tValidation Loss: 1.485345\n",
      "Epoch: 306 \tTraining Loss: 2.675084 \tValidation Loss: 1.933948\n",
      "Epoch: 307 \tTraining Loss: 2.883298 \tValidation Loss: 1.577317\n",
      "Epoch: 308 \tTraining Loss: 2.700620 \tValidation Loss: 1.655691\n",
      "Epoch: 309 \tTraining Loss: 2.671769 \tValidation Loss: 1.741366\n",
      "Epoch: 310 \tTraining Loss: 3.025111 \tValidation Loss: 1.605952\n",
      "Epoch: 311 \tTraining Loss: 2.569018 \tValidation Loss: 1.618706\n",
      "Epoch: 312 \tTraining Loss: 2.122821 \tValidation Loss: 1.545147\n",
      "Epoch: 313 \tTraining Loss: 2.441429 \tValidation Loss: 1.468834\n",
      "Validation loss decreased (1.480167 --> 1.468834).  Saving model ...\n",
      "Epoch: 314 \tTraining Loss: 2.328098 \tValidation Loss: 1.517819\n",
      "Epoch: 315 \tTraining Loss: 2.488104 \tValidation Loss: 1.577984\n",
      "Epoch: 316 \tTraining Loss: 2.121965 \tValidation Loss: 1.746037\n",
      "Epoch: 317 \tTraining Loss: 2.418359 \tValidation Loss: 1.526859\n",
      "Epoch: 318 \tTraining Loss: 2.382782 \tValidation Loss: 1.482064\n",
      "Epoch: 319 \tTraining Loss: 2.663476 \tValidation Loss: 1.548873\n",
      "Epoch: 320 \tTraining Loss: 2.649472 \tValidation Loss: 1.646377\n",
      "Epoch: 321 \tTraining Loss: 2.687983 \tValidation Loss: 1.438120\n",
      "Validation loss decreased (1.468834 --> 1.438120).  Saving model ...\n",
      "Epoch: 322 \tTraining Loss: 2.154096 \tValidation Loss: 1.449284\n",
      "Epoch: 323 \tTraining Loss: 3.040922 \tValidation Loss: 1.586670\n",
      "Epoch: 324 \tTraining Loss: 2.528249 \tValidation Loss: 1.619201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325 \tTraining Loss: 2.835530 \tValidation Loss: 1.488580\n",
      "Epoch: 326 \tTraining Loss: 2.831465 \tValidation Loss: 1.757023\n",
      "Epoch: 327 \tTraining Loss: 2.609312 \tValidation Loss: 1.600014\n",
      "Epoch: 328 \tTraining Loss: 2.781327 \tValidation Loss: 1.568379\n",
      "Epoch: 329 \tTraining Loss: 2.461614 \tValidation Loss: 1.540522\n",
      "Epoch: 330 \tTraining Loss: 2.497802 \tValidation Loss: 1.461185\n",
      "Epoch: 331 \tTraining Loss: 2.800539 \tValidation Loss: 1.632492\n",
      "Epoch: 332 \tTraining Loss: 2.343809 \tValidation Loss: 1.489179\n",
      "Epoch: 333 \tTraining Loss: 2.603574 \tValidation Loss: 1.448491\n",
      "Epoch: 334 \tTraining Loss: 2.497686 \tValidation Loss: 1.459209\n",
      "Epoch: 335 \tTraining Loss: 2.368645 \tValidation Loss: 1.469280\n",
      "Epoch: 336 \tTraining Loss: 2.349399 \tValidation Loss: 1.510553\n",
      "Epoch: 337 \tTraining Loss: 2.532510 \tValidation Loss: 1.445330\n",
      "Epoch: 338 \tTraining Loss: 3.141379 \tValidation Loss: 1.638880\n",
      "Epoch: 339 \tTraining Loss: 2.713323 \tValidation Loss: 1.535034\n",
      "Epoch: 340 \tTraining Loss: 2.510427 \tValidation Loss: 1.509707\n",
      "Epoch: 341 \tTraining Loss: 2.315762 \tValidation Loss: 1.437740\n",
      "Validation loss decreased (1.438120 --> 1.437740).  Saving model ...\n",
      "Epoch: 342 \tTraining Loss: 2.360176 \tValidation Loss: 1.482846\n",
      "Epoch: 343 \tTraining Loss: 2.259655 \tValidation Loss: 1.391532\n",
      "Validation loss decreased (1.437740 --> 1.391532).  Saving model ...\n",
      "Epoch: 344 \tTraining Loss: 2.727382 \tValidation Loss: 1.595622\n",
      "Epoch: 345 \tTraining Loss: 2.578239 \tValidation Loss: 1.479207\n",
      "Epoch: 346 \tTraining Loss: 2.571015 \tValidation Loss: 1.509441\n",
      "Epoch: 347 \tTraining Loss: 2.875794 \tValidation Loss: 1.456380\n",
      "Epoch: 348 \tTraining Loss: 2.332133 \tValidation Loss: 1.405153\n",
      "Epoch: 349 \tTraining Loss: 2.256645 \tValidation Loss: 1.412709\n",
      "Epoch: 350 \tTraining Loss: 2.909922 \tValidation Loss: 1.461926\n",
      "Epoch: 351 \tTraining Loss: 2.290077 \tValidation Loss: 1.438084\n",
      "Epoch: 352 \tTraining Loss: 2.510102 \tValidation Loss: 1.510247\n",
      "Epoch: 353 \tTraining Loss: 2.243986 \tValidation Loss: 1.450085\n",
      "Epoch: 354 \tTraining Loss: 2.472939 \tValidation Loss: 1.434772\n",
      "Epoch: 355 \tTraining Loss: 2.405293 \tValidation Loss: 1.384709\n",
      "Validation loss decreased (1.391532 --> 1.384709).  Saving model ...\n",
      "Epoch: 356 \tTraining Loss: 2.363998 \tValidation Loss: 1.383536\n",
      "Validation loss decreased (1.384709 --> 1.383536).  Saving model ...\n",
      "Epoch: 357 \tTraining Loss: 2.077545 \tValidation Loss: 1.405885\n",
      "Epoch: 358 \tTraining Loss: 2.055721 \tValidation Loss: 1.386605\n",
      "Epoch: 359 \tTraining Loss: 2.715395 \tValidation Loss: 1.419629\n",
      "Epoch: 360 \tTraining Loss: 2.756310 \tValidation Loss: 1.446300\n",
      "Epoch: 361 \tTraining Loss: 2.805498 \tValidation Loss: 1.603251\n",
      "Epoch: 362 \tTraining Loss: 2.582556 \tValidation Loss: 1.506115\n",
      "Epoch: 363 \tTraining Loss: 2.375184 \tValidation Loss: 1.480554\n",
      "Epoch: 364 \tTraining Loss: 2.666904 \tValidation Loss: 1.508595\n",
      "Epoch: 365 \tTraining Loss: 2.304401 \tValidation Loss: 1.431303\n",
      "Epoch: 366 \tTraining Loss: 2.847722 \tValidation Loss: 1.541186\n",
      "Epoch: 367 \tTraining Loss: 2.565519 \tValidation Loss: 1.614246\n",
      "Epoch: 368 \tTraining Loss: 2.439150 \tValidation Loss: 1.429343\n",
      "Epoch: 369 \tTraining Loss: 2.333278 \tValidation Loss: 1.399949\n",
      "Epoch: 370 \tTraining Loss: 2.560099 \tValidation Loss: 1.434894\n",
      "Epoch: 371 \tTraining Loss: 2.681679 \tValidation Loss: 1.465217\n",
      "Epoch: 372 \tTraining Loss: 2.415481 \tValidation Loss: 1.513823\n",
      "Epoch: 373 \tTraining Loss: 2.766743 \tValidation Loss: 1.466792\n",
      "Epoch: 374 \tTraining Loss: 2.800766 \tValidation Loss: 1.743880\n",
      "Epoch: 375 \tTraining Loss: 2.367475 \tValidation Loss: 1.489209\n",
      "Epoch: 376 \tTraining Loss: 2.573997 \tValidation Loss: 1.456680\n",
      "Epoch: 377 \tTraining Loss: 2.584931 \tValidation Loss: 1.507126\n",
      "Epoch: 378 \tTraining Loss: 2.584925 \tValidation Loss: 1.431625\n",
      "Epoch: 379 \tTraining Loss: 2.634484 \tValidation Loss: 1.455189\n",
      "Epoch: 380 \tTraining Loss: 2.649122 \tValidation Loss: 1.494643\n",
      "Epoch: 381 \tTraining Loss: 2.654972 \tValidation Loss: 1.399781\n",
      "Epoch: 382 \tTraining Loss: 2.300947 \tValidation Loss: 1.479127\n",
      "Epoch: 383 \tTraining Loss: 2.417596 \tValidation Loss: 1.402297\n",
      "Epoch: 384 \tTraining Loss: 2.415573 \tValidation Loss: 1.379602\n",
      "Validation loss decreased (1.383536 --> 1.379602).  Saving model ...\n",
      "Epoch: 385 \tTraining Loss: 2.532191 \tValidation Loss: 1.430084\n",
      "Epoch: 386 \tTraining Loss: 2.431977 \tValidation Loss: 1.475719\n",
      "Epoch: 387 \tTraining Loss: 2.298880 \tValidation Loss: 1.536757\n",
      "Epoch: 388 \tTraining Loss: 2.219322 \tValidation Loss: 1.440891\n",
      "Epoch: 389 \tTraining Loss: 2.976491 \tValidation Loss: 1.432730\n",
      "Epoch: 390 \tTraining Loss: 2.633323 \tValidation Loss: 1.490818\n",
      "Epoch: 391 \tTraining Loss: 2.639110 \tValidation Loss: 1.557403\n",
      "Epoch: 392 \tTraining Loss: 2.326821 \tValidation Loss: 1.495862\n",
      "Epoch: 393 \tTraining Loss: 2.547525 \tValidation Loss: 1.627628\n",
      "Epoch: 394 \tTraining Loss: 2.694096 \tValidation Loss: 1.522759\n",
      "Epoch: 395 \tTraining Loss: 2.398821 \tValidation Loss: 1.562166\n",
      "Epoch: 396 \tTraining Loss: 2.374746 \tValidation Loss: 1.555032\n",
      "Epoch: 397 \tTraining Loss: 2.795529 \tValidation Loss: 1.418445\n",
      "Epoch: 398 \tTraining Loss: 2.103401 \tValidation Loss: 1.419144\n",
      "Epoch: 399 \tTraining Loss: 2.388308 \tValidation Loss: 1.731827\n",
      "Epoch: 400 \tTraining Loss: 2.581616 \tValidation Loss: 1.474482\n",
      "Epoch: 401 \tTraining Loss: 2.162748 \tValidation Loss: 1.411431\n",
      "Epoch: 402 \tTraining Loss: 2.052233 \tValidation Loss: 1.431065\n",
      "Epoch: 403 \tTraining Loss: 2.609955 \tValidation Loss: 1.393335\n",
      "Epoch: 404 \tTraining Loss: 2.300717 \tValidation Loss: 1.371713\n",
      "Validation loss decreased (1.379602 --> 1.371713).  Saving model ...\n",
      "Epoch: 405 \tTraining Loss: 2.138720 \tValidation Loss: 1.360061\n",
      "Validation loss decreased (1.371713 --> 1.360061).  Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 2.440507 \tValidation Loss: 1.392187\n",
      "Epoch: 407 \tTraining Loss: 2.378639 \tValidation Loss: 1.339617\n",
      "Validation loss decreased (1.360061 --> 1.339617).  Saving model ...\n",
      "Epoch: 408 \tTraining Loss: 2.511592 \tValidation Loss: 1.445414\n",
      "Epoch: 409 \tTraining Loss: 2.386678 \tValidation Loss: 1.349529\n",
      "Epoch: 410 \tTraining Loss: 2.077950 \tValidation Loss: 1.402078\n",
      "Epoch: 411 \tTraining Loss: 2.478032 \tValidation Loss: 1.441713\n",
      "Epoch: 412 \tTraining Loss: 2.397193 \tValidation Loss: 1.393295\n",
      "Epoch: 413 \tTraining Loss: 2.297153 \tValidation Loss: 1.394489\n",
      "Epoch: 414 \tTraining Loss: 2.346167 \tValidation Loss: 1.364858\n",
      "Epoch: 415 \tTraining Loss: 2.686494 \tValidation Loss: 1.497455\n",
      "Epoch: 416 \tTraining Loss: 2.401476 \tValidation Loss: 1.547609\n",
      "Epoch: 417 \tTraining Loss: 2.282754 \tValidation Loss: 1.388357\n",
      "Epoch: 418 \tTraining Loss: 2.042660 \tValidation Loss: 1.445064\n",
      "Epoch: 419 \tTraining Loss: 2.263374 \tValidation Loss: 1.342728\n",
      "Epoch: 420 \tTraining Loss: 2.259003 \tValidation Loss: 1.421389\n",
      "Epoch: 421 \tTraining Loss: 2.513258 \tValidation Loss: 1.409129\n",
      "Epoch: 422 \tTraining Loss: 2.425456 \tValidation Loss: 1.381849\n",
      "Epoch: 423 \tTraining Loss: 2.212311 \tValidation Loss: 1.373577\n",
      "Epoch: 424 \tTraining Loss: 2.388529 \tValidation Loss: 1.507350\n",
      "Epoch: 425 \tTraining Loss: 2.365098 \tValidation Loss: 1.393395\n",
      "Epoch: 426 \tTraining Loss: 2.251086 \tValidation Loss: 1.371920\n",
      "Epoch: 427 \tTraining Loss: 2.364998 \tValidation Loss: 1.546998\n",
      "Epoch: 428 \tTraining Loss: 2.360218 \tValidation Loss: 1.390054\n",
      "Epoch: 429 \tTraining Loss: 2.640937 \tValidation Loss: 1.419604\n",
      "Epoch: 430 \tTraining Loss: 2.396465 \tValidation Loss: 1.510081\n",
      "Epoch: 431 \tTraining Loss: 2.449336 \tValidation Loss: 1.458826\n",
      "Epoch: 432 \tTraining Loss: 2.103868 \tValidation Loss: 1.356452\n",
      "Epoch: 433 \tTraining Loss: 2.481419 \tValidation Loss: 1.474851\n",
      "Epoch: 434 \tTraining Loss: 2.540038 \tValidation Loss: 1.483708\n",
      "Epoch: 435 \tTraining Loss: 2.753726 \tValidation Loss: 1.421321\n",
      "Epoch: 436 \tTraining Loss: 2.461556 \tValidation Loss: 1.420916\n",
      "Epoch: 437 \tTraining Loss: 2.670926 \tValidation Loss: 1.486908\n",
      "Epoch: 438 \tTraining Loss: 2.615297 \tValidation Loss: 1.410990\n",
      "Epoch: 439 \tTraining Loss: 2.180035 \tValidation Loss: 1.418006\n",
      "Epoch: 440 \tTraining Loss: 2.470279 \tValidation Loss: 1.403981\n",
      "Epoch: 441 \tTraining Loss: 2.533634 \tValidation Loss: 1.372934\n",
      "Epoch: 442 \tTraining Loss: 2.555732 \tValidation Loss: 1.466044\n",
      "Epoch: 443 \tTraining Loss: 2.103807 \tValidation Loss: 1.408884\n",
      "Epoch: 444 \tTraining Loss: 2.469566 \tValidation Loss: 1.390407\n",
      "Epoch: 445 \tTraining Loss: 2.103983 \tValidation Loss: 1.411827\n",
      "Epoch: 446 \tTraining Loss: 2.254910 \tValidation Loss: 1.370038\n",
      "Epoch: 447 \tTraining Loss: 2.248838 \tValidation Loss: 1.346724\n",
      "Epoch: 448 \tTraining Loss: 2.834898 \tValidation Loss: 1.548188\n",
      "Epoch: 449 \tTraining Loss: 2.372888 \tValidation Loss: 1.345807\n",
      "Epoch: 450 \tTraining Loss: 2.599226 \tValidation Loss: 1.362819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 451 \tTraining Loss: 2.153910 \tValidation Loss: 1.424223\n",
      "Epoch: 452 \tTraining Loss: 2.458641 \tValidation Loss: 1.354510\n",
      "Epoch: 453 \tTraining Loss: 2.250051 \tValidation Loss: 1.357955\n",
      "Epoch: 454 \tTraining Loss: 2.394016 \tValidation Loss: 1.366897\n",
      "Epoch: 455 \tTraining Loss: 2.162850 \tValidation Loss: 1.541175\n",
      "Epoch: 456 \tTraining Loss: 2.674323 \tValidation Loss: 1.420804\n",
      "Epoch: 457 \tTraining Loss: 2.655218 \tValidation Loss: 1.403281\n",
      "Epoch: 458 \tTraining Loss: 2.483267 \tValidation Loss: 1.390970\n",
      "Epoch: 459 \tTraining Loss: 2.781424 \tValidation Loss: 1.420856\n",
      "Epoch: 460 \tTraining Loss: 2.444321 \tValidation Loss: 1.421001\n",
      "Epoch: 461 \tTraining Loss: 2.489807 \tValidation Loss: 1.553454\n",
      "Epoch: 462 \tTraining Loss: 2.423725 \tValidation Loss: 1.430093\n",
      "Epoch: 463 \tTraining Loss: 2.003889 \tValidation Loss: 1.395496\n",
      "Epoch: 464 \tTraining Loss: 2.279734 \tValidation Loss: 1.345935\n",
      "Epoch: 465 \tTraining Loss: 2.105867 \tValidation Loss: 1.395441\n",
      "Epoch: 466 \tTraining Loss: 2.606009 \tValidation Loss: 1.442733\n",
      "Epoch: 467 \tTraining Loss: 2.300741 \tValidation Loss: 1.336735\n",
      "Validation loss decreased (1.339617 --> 1.336735).  Saving model ...\n",
      "Epoch: 468 \tTraining Loss: 2.661217 \tValidation Loss: 1.413788\n",
      "Epoch: 469 \tTraining Loss: 2.273352 \tValidation Loss: 1.410325\n",
      "Epoch: 470 \tTraining Loss: 2.248019 \tValidation Loss: 1.344656\n",
      "Epoch: 471 \tTraining Loss: 2.159410 \tValidation Loss: 1.395360\n",
      "Epoch: 472 \tTraining Loss: 2.834964 \tValidation Loss: 1.435327\n",
      "Epoch: 473 \tTraining Loss: 2.389210 \tValidation Loss: 1.465030\n",
      "Epoch: 474 \tTraining Loss: 2.485231 \tValidation Loss: 1.420429\n",
      "Epoch: 475 \tTraining Loss: 2.100606 \tValidation Loss: 1.377869\n",
      "Epoch: 476 \tTraining Loss: 2.219257 \tValidation Loss: 1.423188\n",
      "Epoch: 477 \tTraining Loss: 2.369529 \tValidation Loss: 1.416312\n",
      "Epoch: 478 \tTraining Loss: 2.045967 \tValidation Loss: 1.381245\n",
      "Epoch: 479 \tTraining Loss: 2.558896 \tValidation Loss: 1.397019\n",
      "Epoch: 480 \tTraining Loss: 2.064899 \tValidation Loss: 1.370541\n",
      "Epoch: 481 \tTraining Loss: 2.458940 \tValidation Loss: 1.337523\n",
      "Epoch: 482 \tTraining Loss: 2.518243 \tValidation Loss: 1.397897\n",
      "Epoch: 483 \tTraining Loss: 2.604297 \tValidation Loss: 1.393816\n",
      "Epoch: 484 \tTraining Loss: 2.202928 \tValidation Loss: 1.323416\n",
      "Validation loss decreased (1.336735 --> 1.323416).  Saving model ...\n",
      "Epoch: 485 \tTraining Loss: 2.485675 \tValidation Loss: 1.362200\n",
      "Epoch: 486 \tTraining Loss: 2.713644 \tValidation Loss: 1.351795\n",
      "Epoch: 487 \tTraining Loss: 2.266511 \tValidation Loss: 1.345052\n",
      "Epoch: 488 \tTraining Loss: 2.556755 \tValidation Loss: 1.370275\n",
      "Epoch: 489 \tTraining Loss: 2.459061 \tValidation Loss: 1.339852\n",
      "Epoch: 490 \tTraining Loss: 2.291089 \tValidation Loss: 1.339167\n",
      "Epoch: 491 \tTraining Loss: 2.067245 \tValidation Loss: 1.302394\n",
      "Validation loss decreased (1.323416 --> 1.302394).  Saving model ...\n",
      "Epoch: 492 \tTraining Loss: 2.028621 \tValidation Loss: 1.298385\n",
      "Validation loss decreased (1.302394 --> 1.298385).  Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 2.212657 \tValidation Loss: 1.381557\n",
      "Epoch: 494 \tTraining Loss: 2.685350 \tValidation Loss: 1.356096\n",
      "Epoch: 495 \tTraining Loss: 2.400926 \tValidation Loss: 1.337868\n",
      "Epoch: 496 \tTraining Loss: 2.176395 \tValidation Loss: 1.360246\n",
      "Epoch: 497 \tTraining Loss: 2.020569 \tValidation Loss: 1.449259\n",
      "Epoch: 498 \tTraining Loss: 2.575273 \tValidation Loss: 1.348178\n",
      "Epoch: 499 \tTraining Loss: 1.901724 \tValidation Loss: 1.300986\n",
      "Epoch: 500 \tTraining Loss: 2.370718 \tValidation Loss: 1.619498\n",
      "Epoch: 501 \tTraining Loss: 2.621378 \tValidation Loss: 1.473634\n",
      "Epoch: 502 \tTraining Loss: 2.435751 \tValidation Loss: 1.381260\n",
      "Epoch: 503 \tTraining Loss: 2.594578 \tValidation Loss: 1.553660\n",
      "Epoch: 504 \tTraining Loss: 2.546112 \tValidation Loss: 1.454170\n",
      "Epoch: 505 \tTraining Loss: 2.424069 \tValidation Loss: 1.395358\n",
      "Epoch: 506 \tTraining Loss: 2.445247 \tValidation Loss: 1.343582\n",
      "Epoch: 507 \tTraining Loss: 2.673606 \tValidation Loss: 1.387558\n",
      "Epoch: 508 \tTraining Loss: 2.752780 \tValidation Loss: 1.376000\n",
      "Epoch: 509 \tTraining Loss: 1.967687 \tValidation Loss: 1.285984\n",
      "Validation loss decreased (1.298385 --> 1.285984).  Saving model ...\n",
      "Epoch: 510 \tTraining Loss: 1.997627 \tValidation Loss: 1.280610\n",
      "Validation loss decreased (1.285984 --> 1.280610).  Saving model ...\n",
      "Epoch: 511 \tTraining Loss: 2.313816 \tValidation Loss: 1.418870\n",
      "Epoch: 512 \tTraining Loss: 2.522130 \tValidation Loss: 1.390906\n",
      "Epoch: 513 \tTraining Loss: 1.829270 \tValidation Loss: 1.341700\n",
      "Epoch: 514 \tTraining Loss: 2.322537 \tValidation Loss: 1.319858\n",
      "Epoch: 515 \tTraining Loss: 2.137950 \tValidation Loss: 1.286452\n",
      "Epoch: 516 \tTraining Loss: 2.402339 \tValidation Loss: 1.465192\n",
      "Epoch: 517 \tTraining Loss: 2.296336 \tValidation Loss: 1.369476\n",
      "Epoch: 518 \tTraining Loss: 2.282039 \tValidation Loss: 1.307733\n",
      "Epoch: 519 \tTraining Loss: 2.309058 \tValidation Loss: 1.309722\n",
      "Epoch: 520 \tTraining Loss: 2.499847 \tValidation Loss: 1.316037\n",
      "Epoch: 521 \tTraining Loss: 2.131485 \tValidation Loss: 1.292544\n",
      "Epoch: 522 \tTraining Loss: 2.652233 \tValidation Loss: 1.333991\n",
      "Epoch: 523 \tTraining Loss: 2.029844 \tValidation Loss: 1.290006\n",
      "Epoch: 524 \tTraining Loss: 2.023843 \tValidation Loss: 1.273618\n",
      "Validation loss decreased (1.280610 --> 1.273618).  Saving model ...\n",
      "Epoch: 525 \tTraining Loss: 2.292826 \tValidation Loss: 1.293746\n",
      "Epoch: 526 \tTraining Loss: 2.111124 \tValidation Loss: 1.299833\n",
      "Epoch: 527 \tTraining Loss: 2.369365 \tValidation Loss: 1.306851\n",
      "Epoch: 528 \tTraining Loss: 2.206354 \tValidation Loss: 1.329248\n",
      "Epoch: 529 \tTraining Loss: 2.393255 \tValidation Loss: 1.339638\n",
      "Epoch: 530 \tTraining Loss: 1.885970 \tValidation Loss: 1.306296\n",
      "Epoch: 531 \tTraining Loss: 2.171078 \tValidation Loss: 1.373177\n",
      "Epoch: 532 \tTraining Loss: 2.290078 \tValidation Loss: 1.296802\n",
      "Epoch: 533 \tTraining Loss: 2.580913 \tValidation Loss: 1.311771\n",
      "Epoch: 534 \tTraining Loss: 2.336682 \tValidation Loss: 1.512712\n",
      "Epoch: 535 \tTraining Loss: 2.172252 \tValidation Loss: 1.295465\n",
      "Epoch: 536 \tTraining Loss: 1.956201 \tValidation Loss: 1.287611\n",
      "Epoch: 537 \tTraining Loss: 2.314821 \tValidation Loss: 1.328486\n",
      "Epoch: 538 \tTraining Loss: 2.608461 \tValidation Loss: 1.320205\n",
      "Epoch: 539 \tTraining Loss: 1.825312 \tValidation Loss: 1.295437\n",
      "Epoch: 540 \tTraining Loss: 1.997746 \tValidation Loss: 1.299076\n",
      "Epoch: 541 \tTraining Loss: 2.298117 \tValidation Loss: 1.305764\n",
      "Epoch: 542 \tTraining Loss: 2.298100 \tValidation Loss: 1.336664\n",
      "Epoch: 543 \tTraining Loss: 2.381382 \tValidation Loss: 1.317523\n",
      "Epoch: 544 \tTraining Loss: 2.316311 \tValidation Loss: 1.334189\n",
      "Epoch: 545 \tTraining Loss: 2.479951 \tValidation Loss: 1.366974\n",
      "Epoch: 546 \tTraining Loss: 2.490726 \tValidation Loss: 1.361388\n",
      "Epoch: 547 \tTraining Loss: 2.269093 \tValidation Loss: 1.351876\n",
      "Epoch: 548 \tTraining Loss: 2.155747 \tValidation Loss: 1.354862\n",
      "Epoch: 549 \tTraining Loss: 2.479377 \tValidation Loss: 1.474006\n",
      "Epoch: 550 \tTraining Loss: 2.709747 \tValidation Loss: 1.405521\n",
      "Epoch: 551 \tTraining Loss: 2.362178 \tValidation Loss: 1.382793\n",
      "Epoch: 552 \tTraining Loss: 2.049155 \tValidation Loss: 1.307110\n",
      "Epoch: 553 \tTraining Loss: 1.974112 \tValidation Loss: 1.301646\n",
      "Epoch: 554 \tTraining Loss: 2.328397 \tValidation Loss: 1.287715\n",
      "Epoch: 555 \tTraining Loss: 2.350584 \tValidation Loss: 1.290004\n",
      "Epoch: 556 \tTraining Loss: 1.992902 \tValidation Loss: 1.381048\n",
      "Epoch: 557 \tTraining Loss: 2.386753 \tValidation Loss: 1.501258\n",
      "Epoch: 558 \tTraining Loss: 2.695964 \tValidation Loss: 1.366100\n",
      "Epoch: 559 \tTraining Loss: 2.231145 \tValidation Loss: 1.448082\n",
      "Epoch: 560 \tTraining Loss: 2.734898 \tValidation Loss: 1.410215\n",
      "Epoch: 561 \tTraining Loss: 2.477074 \tValidation Loss: 1.370674\n",
      "Epoch: 562 \tTraining Loss: 2.049517 \tValidation Loss: 1.311812\n",
      "Epoch: 563 \tTraining Loss: 2.511216 \tValidation Loss: 1.351408\n",
      "Epoch: 564 \tTraining Loss: 2.313033 \tValidation Loss: 1.368306\n",
      "Epoch: 565 \tTraining Loss: 1.781801 \tValidation Loss: 1.257323\n",
      "Validation loss decreased (1.273618 --> 1.257323).  Saving model ...\n",
      "Epoch: 566 \tTraining Loss: 2.381139 \tValidation Loss: 1.346256\n",
      "Epoch: 567 \tTraining Loss: 2.009976 \tValidation Loss: 1.320486\n",
      "Epoch: 568 \tTraining Loss: 2.250374 \tValidation Loss: 1.272437\n",
      "Epoch: 569 \tTraining Loss: 2.505128 \tValidation Loss: 1.290271\n",
      "Epoch: 570 \tTraining Loss: 2.203910 \tValidation Loss: 1.320990\n",
      "Epoch: 571 \tTraining Loss: 2.238981 \tValidation Loss: 1.302628\n",
      "Epoch: 572 \tTraining Loss: 2.508940 \tValidation Loss: 1.306491\n",
      "Epoch: 573 \tTraining Loss: 2.284336 \tValidation Loss: 1.315639\n",
      "Epoch: 574 \tTraining Loss: 2.262879 \tValidation Loss: 1.269233\n",
      "Epoch: 575 \tTraining Loss: 2.569906 \tValidation Loss: 1.335351\n",
      "Epoch: 576 \tTraining Loss: 2.457811 \tValidation Loss: 1.343921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 577 \tTraining Loss: 1.804058 \tValidation Loss: 1.347906\n",
      "Epoch: 578 \tTraining Loss: 2.363576 \tValidation Loss: 1.308756\n",
      "Epoch: 579 \tTraining Loss: 2.101398 \tValidation Loss: 1.307961\n",
      "Epoch: 580 \tTraining Loss: 2.104704 \tValidation Loss: 1.404004\n",
      "Epoch: 581 \tTraining Loss: 2.544201 \tValidation Loss: 1.339925\n",
      "Epoch: 582 \tTraining Loss: 2.443305 \tValidation Loss: 1.308572\n",
      "Epoch: 583 \tTraining Loss: 2.407785 \tValidation Loss: 1.418116\n",
      "Epoch: 584 \tTraining Loss: 2.105548 \tValidation Loss: 1.327749\n",
      "Epoch: 585 \tTraining Loss: 2.112252 \tValidation Loss: 1.301777\n",
      "Epoch: 586 \tTraining Loss: 2.640591 \tValidation Loss: 1.448582\n",
      "Epoch: 587 \tTraining Loss: 2.262759 \tValidation Loss: 1.379649\n",
      "Epoch: 588 \tTraining Loss: 2.055375 \tValidation Loss: 1.297869\n",
      "Epoch: 589 \tTraining Loss: 2.384824 \tValidation Loss: 1.282823\n",
      "Epoch: 590 \tTraining Loss: 2.441805 \tValidation Loss: 1.354588\n",
      "Epoch: 591 \tTraining Loss: 2.192901 \tValidation Loss: 1.297990\n",
      "Epoch: 592 \tTraining Loss: 2.238631 \tValidation Loss: 1.285679\n",
      "Epoch: 593 \tTraining Loss: 2.325571 \tValidation Loss: 1.304746\n",
      "Epoch: 594 \tTraining Loss: 2.273883 \tValidation Loss: 1.323926\n",
      "Epoch: 595 \tTraining Loss: 2.281569 \tValidation Loss: 1.341012\n",
      "Epoch: 596 \tTraining Loss: 2.281208 \tValidation Loss: 1.317101\n",
      "Epoch: 597 \tTraining Loss: 2.538600 \tValidation Loss: 1.341671\n",
      "Epoch: 598 \tTraining Loss: 2.283205 \tValidation Loss: 1.302155\n",
      "Epoch: 599 \tTraining Loss: 2.314680 \tValidation Loss: 1.394094\n",
      "Epoch: 600 \tTraining Loss: 2.603570 \tValidation Loss: 1.372812\n"
     ]
    }
   ],
   "source": [
    "#Defining architecture\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        #print(\"Output: \",output.shape)\n",
    "        #print(output)\n",
    "        #print(\"Target:\",target.shape)\n",
    "        #print(target)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "    #for i in range(len(valid_target)):\n",
    "        #data = train_data.iloc[i].to_numpy() #convertir fila de Dataframe a arreglo\n",
    "        #target = np.reshape(train_target.to_numpy(),(-1,1)) #cambiar arreglo a matrix nx1\n",
    "        #data = torch.from_numpy(data)\n",
    "        #target = torch.from_numpy(target)\n",
    "        \n",
    "        # move tensors to GPU if CUDA is available\n",
    "        #if train_on_gpu:\n",
    "        #    data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'modelo_clasificacion.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef271d5",
   "metadata": {},
   "source": [
    "## Graphing Training and Validation Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "091dd612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21f05af1c40>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABMKUlEQVR4nO2dd3gV1daH352T3igJvYWO1AABaSIgKnZF7CLYe70WvBZQr9fGtfB5UbGg14YVC4pKLyIl9N5DEQihBBJSz8l8f+wzp85JQgrJCet9njxnZs+emb1Pkt+sWXvttZVhGAiCIAjBR0hVN0AQBEEoGyLggiAIQYoIuCAIQpAiAi4IghCkiIALgiAEKaGn8maJiYlGUlLSqbylIAhC0LN8+fJDhmHU8y0/pQKelJREamrqqbylIAhC0KOU2mVVLi4UQRCEIEUEXBAEIUgRARcEQQhSRMAFQRCClBIFXCn1kVLqoFJqnUdZXaXUDKXUVudnncptpiAIguBLaSzwj4FhPmVjgFmGYbQFZjn3BUEQhFNIiQJuGMZ84IhP8WXAJ87tT4DLK7ZZgiAIQkmU1QfewDCM/QDOz/qBKiql7lBKpSqlUjMyMsp0s1kb05k4d1vZWioIglBDqfRBTMMwJhmGkWIYRkq9en4TiUrFvC0ZTJq/o4JbJgjCqeDw4cMkJyeTnJxMw4YNadKkiWu/oKCg2HNTU1N54IEHSrxHv379KqStc+fO5eKLL66Qa50KyjoTM10p1cgwjP1KqUbAwYpslC8RoSHkFxZV5i0EQagkEhISWLVqFQDjxo0jNjaWRx991HXcbrcTGmotRSkpKaSkpJR4j0WLFlVIW4ONslrgPwGjnNujgB8rpjnWRITaKHCIgAtCTWH06NE88sgjDB48mCeeeIKlS5fSr18/unfvTr9+/di8eTPgbRGPGzeOW265hUGDBtGqVSsmTJjgul5sbKyr/qBBgxgxYgQdOnTghhtuwFx17Ndff6VDhw4MGDCABx54oERL+8iRI1x++eV07dqVPn36sGbNGgDmzZvneoPo3r07WVlZ7N+/n4EDB5KcnEznzp1ZsGBBhX9nVpRogSulvgQGAYlKqb3AWOBl4Gul1K3AbuCqymxkRGgIjiIDu6OIUJuErgtCWXnu5/Vs2He8Qq/ZsXE8Yy/pdNLnbdmyhZkzZ2Kz2Th+/Djz588nNDSUmTNn8s9//pPvvvvO75xNmzYxZ84csrKyaN++PXfffTdhYWFedVauXMn69etp3Lgx/fv3588//yQlJYU777yT+fPn07JlS6677roS2zd27Fi6d+/ODz/8wOzZs7nppptYtWoV48eP57///S/9+/cnOzubyMhIJk2axPnnn89TTz2Fw+EgJyfnpL+PslCigBuGEain51RwWwISEaZFO98uAi4INYWrrroKm80GwLFjxxg1ahRbt25FKUVhYaHlORdddBERERFERERQv3590tPTadq0qVed3r17u8qSk5NJS0sjNjaWVq1a0bJlSwCuu+46Jk2aVGz7Fi5c6HqIDBkyhMOHD3Ps2DH69+/PI488wg033MDw4cNp2rQpvXr14pZbbqGwsJDLL7+c5OTk8nw1peaUZiMsKxGh+pecby8iJqKKGyMIQUxZLOXKIiYmxrX9zDPPMHjwYKZOnUpaWhqDBg2yPCciwi0ANpsNu91eqjplWbzd6hylFGPGjOGiiy7i119/pU+fPsycOZOBAwcyf/58fvnlF0aOHMljjz3GTTfddNL3PFmCwpwNDzUtcEcVt0QQhMrg2LFjNGnSBICPP/64wq/foUMHduzYQVpaGgBfffVViecMHDiQzz//HNC+9cTEROLj49m+fTtdunThiSeeICUlhU2bNrFr1y7q16/P7bffzq233sqKFSsqvA9WBIkF7hRwiUQRhBrJ448/zqhRo3j99dcZMmRIhV8/KiqKiRMnMmzYMBITE+ndu3eJ54wbN46bb76Zrl27Eh0dzSef6LmLb775JnPmzMFms9GxY0cuuOACpkyZwmuvvUZYWBixsbH873//q/A+WKHK8mpRVlJSUoyyLOjwy5r93PvFCn5/aCDtG8ZVQssEQajpZGdnExsbi2EY3HvvvbRt25aHH364qptVKpRSyw3D8IunDAoXSoS4UARBKCfvv/8+ycnJdOrUiWPHjnHnnXdWdZPKTXC4UDyiUARBEMrCww8/HDQWd2kJEgtcR6EUiIALgiC4CBIBFxeKIAiCL0Eh4HX3zeUO288ShSIIguBBUAh4/N653BP6k/jABUEQPAgKAQ+JjCOaPHGhCEIQMmjQIH7//XevsjfffJN77rmn2HPMkOMLL7yQzMxMvzrjxo1j/Pjxxd77hx9+YMOGDa79Z599lpkzZ55E662pLmlng0PAI2IJVw4K8vOquimCIJwk1113HVOmTPEqmzJlSqkSSoHOIli7du0y3dtXwJ9//nmGDh1apmtVR4JCwMOj4wHIz6nYLGqCIFQ+I0aMYNq0aeTn5wOQlpbGvn37GDBgAHfffTcpKSl06tSJsWPHWp6flJTEoUOHAHjxxRdp3749Q4cOdaWcBR3j3atXL7p168aVV15JTk4OixYt4qeffuKxxx4jOTmZ7du3M3r0aL799lsAZs2aRffu3enSpQu33HKLq31JSUmMHTuWHj160KVLFzZt2lRs/6oy7WxQxIGHRupcv4Ui4IJQPqaPgQNrK/aaDbvABS8HPJyQkEDv3r357bffuOyyy5gyZQrXXHMNSilefPFF6tati8Ph4JxzzmHNmjV07drV8jrLly9nypQprFy5ErvdTo8ePejZsycAw4cP5/bbbwfg6aef5sMPP+T+++/n0ksv5eKLL2bEiBFe18rLy2P06NHMmjWLdu3acdNNN/HOO+/w0EMPAZCYmMiKFSuYOHEi48eP54MPPgjYv6pMOxsUFriK0NPnRcAFITjxdKN4uk++/vprevToQffu3Vm/fr2Xu8OXBQsWcMUVVxAdHU18fDyXXnqp69i6des466yz6NKlC59//jnr168vtj2bN2+mZcuWtGvXDoBRo0Yxf/581/Hhw4cD0LNnT1cCrEAsXLiQkSNHAtZpZydMmEBmZiahoaH06tWLyZMnM27cONauXUtcXPlSgwSFBU64tsAd+dlV3BBBCHKKsZQrk8svv5xHHnmEFStWkJubS48ePdi5cyfjx49n2bJl1KlTh9GjR5OXV/w4l1LKsnz06NH88MMPdOvWjY8//pi5c+cWe52SckCZKWkDpawt6VqnKu1sUFjgRGgBL8rLquKGCIJQFmJjYxk0aBC33HKLy/o+fvw4MTEx1KpVi/T0dKZPn17sNQYOHMjUqVPJzc0lKyuLn3/+2XUsKyuLRo0aUVhY6EoBCxAXF0dWlr9udOjQgbS0NLZt2wbAp59+ytlnn12mvlVl2tkgscB14vecrGMUFRmEhFg/hQVBqL5cd911DB8+3OVK6datG927d6dTp060atWK/v37F3t+jx49uOaaa0hOTqZFixacddZZrmMvvPACZ555Ji1atKBLly4u0b722mu5/fbbmTBhgmvwEiAyMpLJkydz1VVXYbfb6dWrF3fddVeZ+lWVaWeDIp0sR3bChGT+UXAXbc+/g7vObl3xjRMEQaimBHU6WZyDmLEql1kb06u4MYIgCNWD4HChRNWlUIXRSB3mSK2oqm6NIAhCtSA4LPCQEGx1WtAu4ghHTuRXdWsEQRCqBcEh4EBInea0Cj1M+nERcEEQBAgiAadOSxoW7uXIsawSYzgFQRBOB4JHwM+4mMiiE/Qv/IuMbLHCBUEQgkfAWw7CYYugS8hOXv1tM/uP5VZ1iwRBEKqU4BHwkBCM2km0VAf4dvleHpqyqqpbJAiCUKUEj4ADtnptaan2A5BbKIs7CIJwehNUAq7qd6RlSDq1yaJuTHhVN0cQBKFKCSoB54yLseHgzoabOZxdUNWtEQRBqFKCS8AbdoXwWDqF7CYjSyJRBEE4vQkuAVcK6raiSdHfHMrOp6hI4sEFQTh9CS4BB0hoTb38vdiLDIkHFwThtCb4BLxhV+Jz99BRpXHmv2dhdxRVdYsEQRCqhOAT8JRbMJSNYbalAKQdziGv0IFD3CmCIJxmBJ+AR9XGEduQJuoQAJsPZNHhmd949JvVVdwwQRCEU0u5BFwp9bBSar1Sap1S6kulVGRFNazY+9ZuQVOngC/deRiAqSv/PhW3FgRBqDaUWcCVUk2AB4AUwzA6Azbg2opqWHHY6jSnS8wx+rSqy6eLd52KWwqCIFQ7yutCCQWilFKhQDSwr/xNKgUJrYnOPcANnWMR17cgCKcrZRZwwzD+BsYDu4H9wDHDMP6oqIYVS9vzAIPkYzNOye0EQRCqI+VxodQBLgNaAo2BGKXUjRb17lBKpSqlUjMyMsreUk8adYMW/Wm68j+EYq+YawqCIAQZ5XGhDAV2GoaRYRhGIfA90M+3kmEYkwzDSDEMI6VevXrluJ0HSkHP0aiCbJLUgYq5piAIQpBRHgHfDfRRSkUrpRRwDrCxYppVCuqfAUB7tfeU3VIQBKE6UR4f+BLgW2AFsNZ5rUkV1K6SSWwHIWF0D9l6ym4pCIJQnShXFIphGGMNw+hgGEZnwzBGGoZx6pKThEZAu/O5zLYIhUynFwTh9CP4ZmJ60moQ9dQxEjlW1S0RBEE45QS3gNduAUAzVUHRLYIgCEFEkAt4cwCaOgW81ZO/kFsga2UKgnB6EOQC3gxwW+BFBmzPyK7KFgmCIJwyglvAw2PIUrHUV0ddRWmHT1RhgwRBEE4dwS3gQHSdRowKncGNSVkA7MgQARcE4fQg6AXcFhUPwL8OPURSQjQLtmaQbxc/uCAINZ+gF3AKnBa3PZeOjeNZlnaUlBdmsmL30eLPEwRBCHKCX8Dzjrs2r+mlo1Ky8u0Mn7ioqlokCIJwSgh+AW/R17V5dttEujWrXXVtEQRBOIUEv4Bf+n+QfIPezj0KhqzwIAjC6UHwC3h4DLQbpren3kXX/JWuQ7skpFAQhBpM8As4uCb0sPV3Xsh62lX8/M8bqqhBgiAIlU/NEPBazb12zwtZRnu1m0JZMFMQhBpMaFU3oEKIruu1Oyn8DQAuy/21KlojCIJwSqgZFrhSMOQZv+LMnIIqaIwgCMKpoWYIOMDAR+HG77yKjp4QARcEoeZScwQcoM1QiKnv2j2eZ8fukNV6BEGomdQsAQcI8XTrG+w/lldlTREEQahMarSA17Hlc/V7f5Gdb6/CBgmCIFQONU/AbW4B/znpG9KP5bBDFnkQBKEGUvME3MMCb/r3dLqrrbw7bztzNx+swkYJgiBUPDVawAEUBr+uPcDoycuqqEGCIAiVQ80T8MvfgYZdXLtxKrcKGyMIglB51DwBb5wMt8507U4Of43GHKq69giCIFQSNU/AAcIivXZ7hGytooYIgiBUHjVTwAGaneneVBkA3PTRUp78fk1VtUgQBKFCqbkCfusfrs12IXsAmL8lgy+X7qmqFgmCIFQoNVfAPbjC9idDQ5ZXdTMEQRAqlJot4PevYFmEXjPzg/D/VHFjBEEQKpaaLeAJrSmwRVV1KwRBECqFmi3gwJ5ml1V1EwRBECqFGi/gV159Ez9EXEqW4bbEX/t9UxW2SBAEoWKo8QIeZguhQcPGxKlcrrXNBuC/c7ZXcasEQRDKT40XcABlCwfg5bAPXGU3fLCY9+fvqKomCYIglJvTQsAzI5v4lf257TAv/rqxClojCIJQMdSMVelLYE3c2dQp6kBDlVnVTREEQagwymWBK6VqK6W+VUptUkptVEr1raiGVSQnChxsLmpGLSULOwiCUHMorwX+FvCbYRgjlFLhQHQFtKnCySss4hgxxJGDogjj9PAcCYJQwymzkiml4oGBwIcAhmEUGIaRWUHtqlD+cV47mjdpgo0iEjnudzynwE5WXmEVtEwQBKHslMcUbQVkAJOVUiuVUh8opWJ8Kyml7lBKpSqlUjMyMspxu7JTPz6Sy3q1BeCb8Oe8jhmGQcdnf+fCCQuqommCIAhlpjwCHgr0AN4xDKM7cAIY41vJMIxJhmGkGIaRUq9evXLcrpzENQIgKSSdp89IBwwAfl+fDsCeI7JyjyAIwUV5BHwvsNcwjCXO/W/Rgl49aXc+NOsDwG07H2akbQYAHy3cCUCD+Igqa5ogCEJZKLOAG4ZxANijlGrvLDoH2FAhraoMlIIW/Vy7l9kWAbBs1xEACuxFVdIsQRCEslLeKJT7gc+dESg7gJvL36RKJLaBazORYwAYBsRFhJJb6KiqVgmCIJSJcsXTGYaxyunf7moYxuWGYRytqIZVCrH1XZt149wRj8M6NySvsIiiIqMqWiUIglAmTq+AaA8LXIXorndtWovW9WMBuGLin3R4ZnqVNE0QBOFkOS2m0ruIa+jatDl01EnD+EiiwmwArN57rEqaJQiCUBZOMwvc7UJRhVrAG9WKJCrcVlUtEgRBKDOnl4CHx7o2IyngsuTGPDS0ncsCN5mz6eCpbpkgCMJJc3oJuFLuzcIc3rommTox4UT7WOA3f7yMaWv2cSLffqpbKAiCUGpOLwEHSGjj3DDAngfgssC7qB0MDFkNwH1frGT8H5urooWCIAil4vQT8PuXw7BX9PaSdwFcPvCfI57mf+GvuKp6WuB7j+Ywb0vV5HIRBEGw4vQTcIBwZwz4zHEAxEVaB+PUi3NPrz//jfmM+mhpZbdMEASh1JyeAh4a6d4+spO6MRHY8J+J6Tm9/kSBPm4YMtlHEITqwekVB26S5xHvPSGZusD2SP9q2RaDmAWOIiJCJexQEISq5/S0wCNrl1BBW9lZef4CnlcoSa8EQagenJ4C3mUEJJ0V8HA4WrinrdnPA1+u9DqWJ0mvBEGoJpyeAq4UdLg44OEtYwfRr3UCAD+t3ud1zBTwx79dzcKthyqvjYIgCCVwego4QGR84GOFucRGuIcHBr46x7WdV1iE3VHE16l7ufHDJVZnC4IgnBJOXwGPKEbA7bnERYa5dncfyXFt5xY6yJfFHwRBqAacvgIenRD4WGEerer5rc8MaBeK7+o9L0/fxKJt4k4RBOHUcvoKeLPeMPx962OFubRMdAv4OfF7+f52vdxnno8FvvdoDu/O2871H4g7RRCEU8vpK+AhNuh6tfUxey7JzWoDEM8JJhWMoWHaVMDfAp+ydE9lt1QQBMGS01fAi6Mwl8a1o/jrySHEkIeNIiIL9OLHG/ZnkW93hxLuPZoT6CqCIAiVigi4Sd1W7m3XYg9R/PlofwDC7dkATJi1lSyPGZoZ2fmnro2CIAgeiICbxDV2b++cB0Xayg4pKgAgtPCE6/DwiYtc2wePi4ALglA1iIA/th3+sQUi3Kv1sOwDmD9ebztzhofbsyxPFwtcEISqQgQ8JhHiGkB0onf58sngKAS7tsBDCrKZ99ggryqRYSFk5hSeooYKgiB4IwJuEuMj4Fn74c+3wOG0sPOP06R2FG3quy31utHhXqfMlwUfBEE4hYiAm/gKOEDmLpcFTn4WobYQZj5ytutwnRhvAb/po6VMmr9dcoYLgnBKEAE3adLTvyyusdsCzzvudzgmwj+d+r9/3cT6ff51BUEQKprTc0EHK1r0g3uXQlwj2Pgz/HgPHFgLx//Wx/P9BzHTj+dZXupoTkFltlQQBAEQC9ybeu11lsLuN0BMfdj8C6z8VB/LPw5F3jlQQkOU5WUysiQyRRCEykcEPBARcT4FBnjEggNMvKEn/7q8s2vavcm+zNwSL59TYCenwH/FH0EQhNIiAh4Iq3zhR3d57bauF8ONfVr4ZSecuznDr8yX5Odn0PHZ38vdTEEQTl9EwAMRarHK8bt6Wv2/r+hCk9pRhNr01+eZGwUgdddR3p6zrdjLlyTwgiAIJSGDmIE4HECADYPrz2zO9Wc2dxWFhrifg72S6pBb6GD1nkzL09+Zu52oMHluCoJQfkRJAhHX0LrcUQA+cd7vjnSHIMZEhNKsTjR7AmQpfOW3TYz7eUOFNVMQhNMXEfBAXP813LnAv3z6E/BGZ9ibCn8vB6BlYgz3DW4DOAW8bjR7j+biKDLYkZFdqtst2JqBo0gmAAmCUHpEwAMR3xgadfUvXz4Zju+FD86B94fofCmAzRlSGG4LoVmdKArsRbT+568M+c881u87VuytFm0/xMgPl/L27OL95oIgCJ6IgJdEg87FHy/UrpLEuAgAzu3YgE5NanlVSTtU/KIPR07oiT+bDsgMTkEQSk+5BzGVUjYgFfjbMIyLy9+kasatf0B+NuxdBl/d4H986STocAlXp7SlT8u6tG0Q5xdhsiztCPd+sYKG8f6RLYZhEO6MZpHIFEEQToaKsMAfBDZWwHWqJ+ExOt1sWJT18dn/gk+vICLURtsGevJPeGgIDw9t56ry3Yq9ABywmHrvuUByvgi4IAgnQbkEXCnVFLgI+KBimlONCY8JfCxrn19kyu0DW7q2w2yBv+b8wiLynMLtG08uCIJQHOW1wN8EHgdqvunoaYGP/sX/uE+yq+hwt3fK9HFbkWd3kFeghdt0oRiGwZdLd3MsVxaLEAQhMGUWcKXUxcBBwzCWl1DvDqVUqlIqNSMjiBc8CHNa4CoEkgbAef/yPr7iEzhYvCdp6BkN/MryCh3kFmoBN10o6/cd58nv13LtpMXM2XSw/G0XBKFGUh4LvD9wqVIqDZgCDFFKfeZbyTCMSYZhpBiGkVKvXr1y3K6KMS1wc4p9y4Hex/94Gib28Sr65q6+3H6W25VyeffG+JJvL3IJeIG9CLujiMe/XQPAxv3HeeirVZbNycwp4M9th8rQEUEQagplFnDDMJ40DKOpYRhJwLXAbMMwbqywllU3TOGOcT6EwqJLPKVXUl36tEpw7Q/r1JC7B7X2qpNX6CDX6UJxGAZLdh5hw353OOGx3EJL3/gtHy/jhg+WcCJfMhoKwumKxIGXlui6cM6zMHKq3i+FgANEhdtc26G2EOIjw7yOHziWR57TAi+0F1nOxjycrX3oy3cdJStP+8VX7M4EYP+xklPXCoJQM6kQATcMY26NjAH3RCk46x+Q4LSgw4sR8OcT4NtbAe/BTNAhhp7c8elyvl+pV/3JsxdRZLGe5qHsfBZtP8SV7yzivi9Weh3bl2m9KpAgCDUfscDLSlgxYYVFdlj3LQBRYTavQ74CDu4VfDzdKZ7c8nEq17+/BIAVu496WekLtlb8wPAHC3Zw+/9SK/y6giBULCLgZcXmdIU061NstehwbwGPcMaEx5JDHN5T7HMKHHyxdLffNQ5leyzRZkC2h9/7/QU7XS6YimLF7qOs2HW0Qq8pCELFI/nAy4pS8EQaOOwwvo273McFEhVubYGvi7wNgKS8L7yOL9hafGRJVr6dN2Zs8SrLKXAQ6WPpl4XMnAJOFDjIzCms8IeCIAgVj1jg5SGqDkTV9i5zeE++id0zj7TI62mqdDy3rwvlkm6N2fTCMB45tx2l5eNFaQCc1TYRoMLEdtD4ufR/eTZHcwrJsxdhWPjjfZmxIZ1dh0+UWE8QhIpHBLy82LyjSsg+4LUbsf4rAHoqbTX7Jqz6v+u6Exlmo3a0z3VKQT1nBsTcMgr4oex80j3ys2Tm6IfPsZwCHEUGhY6SBfz2/6Vy/pvzy3R/QRDKhwh4RXDp/0GscwWfN7t4HVIh2rVhc2YbyA4Qtx0ZevIukHqxWsB9LfAnv1/D5D93lnj+1e/9xZn/nkWhw/uhciRHhy3mlZCbxTwvr7DmZ1IQhOqICHhF0OMmaNTN+phyCrjSIjeiZ1OeGNbBr1qEc53MMaFf8l342FLd1rTAfQX8y6V7eK6YZds+WZTGJ4vS2JGhXR8LfWZ0moJckmumrJOIDMNg6c4jpXLRCIIQGBHwiiLlFv8yez5smQ5ACFqsIsNsfrMxASKcFvhdoT/TM2Srq7w414rLhVJgbQGv+/sYQ1+f55dMa+xP6xn703paJOhY9p0Z1j7s7QdPYHcEtq4DvU2UxIwN6Vz93l98tsQ/4iYQhY4iMnMCJwUThNMREfCKov0wGOezdNqrrSBXh+PVIxPmvwaFuX6RKuC2wE1WP3se8x8bzG8PeudcGXdJR9d2otOFsvtIDp8v2YVhGJz/htsf/dTUtWw7mE2PF2awJd07WyK4Jxk9P22D5fHr3l/M+D+2+JWblFXAzbzomzxSBpTEg1NWkvz8jDLdTxBqKiLglUmBe0HjexNS9eIPa77SlrmJKeY+ml4rzEHzhGga1nKv4rPuufM5r1ND135CbDgA/5y6lqemrmPD/uNs9hDi1XvdDxQzrtvT7VHg4eOeuTHdsguLtmv3ytq9x8jMKfCaup+dVzYB912BKDvfzlHft4Qf19Huqemu/V/X6sFhcbsIghsR8IrGFmFZHHXcOai44lPXOpqAnrUJfgOJ5Ptbp1FhNupEh7v2Y3ym6f+1/XDAZpnXNwV4TOgX3JPzrut4RIBB1HBbCMfzCrnk7YUkPz+Dvi/NdomopwV+LCdw7vJCRxE/rvrbdZ75tlHgbNPg8XPp/oK3df3JX7socPiHMpYmMkYQThdEwCuaiLjij/+dqtfRNLFrd0KE70ScPP+V7G0hiqhwG09e0IHfHjrLb/LO3M2Bp9Wbwve3M3fKXaHTuNIxnXM76hzlB7Osc6pEhIX4iXOOc7q/p4B3e/4PPyvaZOKc7Tw4ZRW/r/e28k0L3EwlYIVviKS9SCJeBMFEBLyiKU7Am/TUn3Nfcpc53SkD2ybywuWd3eUeAv7d3f3454XuyJU7z25Nh4bxRPr4zRfvCGyBr993nCH/mcsqZxZDk85qB73VRjKOW4toRKjNb2WgLKfrxNeFYtb7cdXfpKYdcZVvTtdvEwU+YYelWQP00W9We+0X2sUCFwQTEfCKpkGnwMcadoXBT3uXOQc1lVKM7NPCXe4h4D1b1OGOgf6RK74WuN0jyVWMzxT+71bsZUfGCSbN3+5V/uD22/k64gUOBrCCw20hHPcTcL3vazmbAv3glFWMePcvV7k5QSgsRAGQ71qByNu6LrJIpWv6vn3vURKFjiL+WH/glPvMD2Xn8+9fNxYbvSMIFYUIeEVzxbuBj8U3hrMf02lpTY7sgOdqw4afvOvuWlTirYpbLDk0wLETFtkOIbALJTw0hEwfAT/utLx/Wr3PqzwnwLVNy9x0uZiWd4HPdP2cAHHnnnU8XShnvzbHLy+MyRsztnDHp8v5c1vgt5LK4LmfNzBp/g7mbQni5QOFoEEEvKKJiIP7V0DXa6HfA97H4pwRJIOehDZD9fY+Z37vb2+BWc+76679JvA99q2Cmc+BYbhiuUHn1zJzrbSuV0y6Wwu2pOuImQfPacudA1u5yh1Fhp8L5fsVezlwLI/tGdle5dl5dq9Ut8fzCnl33naXBf7zmv3YHUUuF0pOgcP1MAB4a6a1GHsOXH6+eDd7juhB4F2Hc3hr1laPekWs2K2jbXYe0rHtN364hIPHKzZn+uo9mQH9/YX20ruHBKG8iIBXBgmtYfh7cN4LcOd8aNpLl4c4o0ZsYdD7Tr2d7Vy0uKgQFvzHfY18/7hsF5MvhIWvw4G1/Pf6Hq7iB4a05b2RPUl7+SJqRQWeAHRRl0YBj91wZnOevPAMrurZFNCzMTN9BjE/X7Kboa/Po8hwhwSCFstxP6137Y/5bg0vT9/E35k68mX+lgxemLbB5TrJyMr3EsL3F+hIHd98MZ4DmW/P2cZHf+60dLe8PH0TwycuYmt6lteDZObGkheGXrT9EF94TCwa99N62vzzV8u6l/33T6567y/LY6E27SayW7RPECoaEfDKplE3OPsJvd0kxV0e5ozvPv639XmFubB3Ofz1X/CLvHCKw3tnUX/7t67Sh89tx+D29Uts0rDODQMei43UD5lXR3QlpUUd8uwO1u3zj4gx3SG+2RW/Tt3j2vb1X4MWU9M6PZiVz2ELS9bX4s8v9PeVe1q4OQW6LanOWPeVuzP5Y4M74mXGhgMs2n6Il6Zv5KEpK/l59T6SxvzCyt3unOcjP1zKP6eudYVZfrwozVKETXfOtoPZfsdARwoBOCo5WmbmhnRS/jVD0v6e5oiAnwrangtjM6GeR8rY8Fj9eTRA0il7LvzxFPz+T1j/vfcxjwyIsUcD5zwBGN6jCV/e7r3oRHEWuLmCkFKKyDAb+4/l8cua/QHrd24S77VfkusgzKa8RGeHjxvm7NfmuFYDGuactOQbSpid7/AaAD2e64yld9776R/XedWfszmD699fwnvzdvDDqn3c/6V2W32zfK+rjjno65uP3XcwsqRBVFPAfd8iDMNg9qb0ChtU/ff0jRzKLmDvUVkT9XRGBPxUoZT3fi3tomD/av+6Jrudr+mZPjlDbO7JPKGO4v+BL+jciL6tE2hWNwqAl4d3ISREBayvPNoZGRbi8iUHYmSfJGY+crblsWtSmvmVafF1i9s2HwHfdTiHVXsyAWjXUIdk+mY7zM4v9LrGC9M2MGfTQddkJV/xDEShRz3zOzl6ooB7v1jhKj+R7/3wKGkiUajzOtke5/2x/gD3fbmSWz5O5ZvUvYFOPSnM7JVigZ/eiIBXFdGJpa/rOyvTY7ZnqD0HK67p1RyAjo21hbzg8SGkvXwR1/ZuXurbRoTZ/NK2dGtay2s/NjKUJrWj/M69bUBLnrvMP6TyeK5e7ScuQrtqlu484lfHpF0D/Zbim4zrRL6DfA9R/2Xtfm7+eJn/bNYS8HSRmEKYuuuo1xvHiQLvWHfPh8NLv270s6hNC9wzZcEdny53XdMcDzAxDAPDMMjMKSBpzC/8ts7f7WSFOQegrBkhy8q2g1m88tumUr9JZOUV8tTUtae8nacLIuBVRchJfPV5vgLudqGEFFoL+LDODUl7+SJLcQ3EWI9EWWCdo/yrO/syaWRP135MuM1rQtGANvrBdGOfFpbLvBU4isjMKaRJnShaJcaw0jmx6Jb+Lf3q1o3RbxrXvb/Yq3zhtkNMW7vPr/7JRn7kFNgpsBeRV+hwWflmhIuJr/B4PiTem7+DjxelcdW7i3h33nbncf80A8Vx+/9SGfnhUtebzsS52wA9wHv+G/OZHyAc0fxuzfGCrelZ3Dx5aaVb5KM+WsY7c7f7zQFYsuOwX1QSwIcLd/L5kt2lyk8vnDwi4NWBG78v/njqh/Cih9/aw4VCwQleuKwTP983oPT3sxhgm3pPP272EdGYCNMfDvMfG8z7N6UQGWajd8u6HnVCvdwu743syYTrupOU6B3GOPnmXrxypV7sYv+xXCLCbLR1WtjgTswFcHlyYxY/eU6x63y++ttmv7L9x04uXPD39en0/vdMLzHyHZz0FWJf98xzP29gWdpRXp6+CXBb8tn5dgzD8PPxe9qt2w5mM3PjQRZuO0So84Ge64yl/2DhDjanZwVMMuYr4E/9sI45mzNcYZTlodBRxHWTFlvO7HW5qXzedq6ZtJhz/jPPr75pqJfWrRUIwzC4/8uVzNlUckRReTmYlceytMBvhtUJEfCq5ME1OmY8srZ3+QWv+tctzIEip3XlJeDZjOybRBfTtfHdbfDnhOLvW+SfeKp7s9p+ZZ2b6GsaBjRPiHblTfEMUfRMqHVR10bERIRyabfGfteKjwylVpRu9/aME7RMiCY2wn2dhBh3n14a3pWGtSJdA6ploU4pl6jLzCl0ZVwE//A/38lJqbuK/8c2BTwrz86Pq/YxxFfUPFwPnlapKcR5dgd5hQ62OuPyfRfFNjG/m+N5dhZszXClAy5uchfAyt1HS3R/7DmSw187DvulMQAIcT6sPb8XRzEhk2HOsMpCizqHs/Np9/R0lng8KLakZ1mK/baD2fy8eh+Pfbum2LafLJ8t3sWY79zX3JGRTe8XZ3HVu38FReZLEfCqpE4LHTNeJ8lddv5LkNDGun5upv70XIezwGeQce03MOOZ4u9rt5g2X+T/6m0umnz9md5+c0+LO9pppe986ULevq673zV+f2gg1/VuTsdGtWhT322V92mVQGyEW5zqOAU8pUUdl2gVZ4GXxMMnsUj07E0HaRgfSYeG/nlsTAvcMAxGT17Kw18VM+iMe8A1IyuPHSUMAHu+Mdz44RJAL85xwwdLmO20NE/k27E7ivh08S4+XOgWfDN8c9WeTEZ+uNQVqx/iO1juwayN6VwxcRFfLdtjeXxLehZ2R5ErtHPv0VymLPUeQPf18c/dfJDWFvHyh7LzefirVS63lpXIL915hAJ7katfGVn5nPfGfMb+tM6vrrlqlO8YjMnczQdZbxHuWhJP/7COKR7fx9DX3Q/c4yeRLrmoyGCrRU79ykYEvDoQk+AemAyPgbBo63o5TkslxEPY8rMgx8IqLM56cFikfl35qV9Ro1pRzHh4IM9d6j8Ymey02GOdg5FKKS9hN2nfMI6XhnchKtxGm/pxrHr2XB4a2pZLujUmxnlumE25ruMZIeNpgRcTOOOFuYJRUkIMr43o6hf8Y8XG/Vk0rBVJnDMGvluz2tw/RD9ETaHKKXAUm+0RtOVqhjzuP5bn9VZhhZW/Oq/QwfJdbjfIiXwHr/6+mWd+WMcL0za4zjFdGD/7pDModBSRnW/nnbnb/UQz7XCOs7/+qYp3H87hvDfm8+rvmzng8WAZ8/1ar3rm0M0S5+DzdovVnPLtDs56ZQ5TV/7tap/vAPO2g9n85bS8zQe1+6Ggv+dZG9O59eNl7MvMZc8RPfhrLmLiy+jJy7howkLLYwDLdx3xetPyxbS2Pb+yw9mBs2T68tGfOzn3jfmuCCpPMrLyeW/ednYfth6vKg8i4NWFDhfpz4g4CA3wj79/NWz6BRwFUKsZdLsesg/Aqy3hiM8gUdYB2D4HxtXyD0N0WEwDn/aQ5S3bNoizfC3/9NbefHVHn5O2kmtHh/PQ0HbERIS6Jg2FKEWR8x8o1EOpI5xWZr/WCcx7bHCprj/xhh7cOqAlfVolcFVKMy4oZtKSye4jOdSKCnM9UJrUjuSmvkmAO3eMbySMFWe9OsclsPuP5Vm6AjwFIq/QQf04b0HyjXqZuvJvJs3f4do3M0EG8ilfO2kxncf+ziu/beKXtd7x+05vBg6Lh/uhE1qsFm0/RLpP6oHMnAKXb9208F+evonMnAK/hGQA/52z3fUgMx/q5sNk56ETbNx/nKGvz+N/f+0CYOWeoxTYi1znZOfbuf/Lldz6SSqzNh2k38uzXZPJSpvMDGB7RjZdxv7OrsMnuPKdv7j+/SUB61oNOltNMgM9MGv6yIuKDIqKDJdwp1m8daUdPsFL0zex+4gIeM1l+CS48kNod35g6/n722DK9VCQA427u3OrAKz6HBwef4TZ6boM/BNjFVjPIiT7YPGWuwdxkWGc2SqhVHUDYVrdRYbh+ge3eQh4nZhwPhqdwjs39vR6iBQnyikt6vLMxR1dLgZzqn+T2lEuC9ukr0f74z0EvFGtKNcArmkVWgn4jIcH+pW5LGR7EXuP+v/Dpu5yL+acby/yymUD1l9/31YJPHmBTic8a2M6936xwjXYWRzHcgvZfTjHFbpoPjxMDZy/JcM1Ucl8MDiK8JsclPz8DIZPXERRkYHN45UmO99Ojk+cvGEYHDnhtlzNB405tjB4/FwueGuB1zl7juTyzA/rXDNqs/Lsfm8WZrhpgb2IqSv3svmA210RyFf93fK9ZOXb+WmVf8SS73m+s38BFmzJ8Pr9n8i3s+1gNi9M28BVzmybncb+zuiPl7kMD98xlPu+WMG1k3QUVZ2Y0o3LnAwi4NUFWxh0GaFdKFF13OUJbaCuTyrZ4/sgNFLXNck6oGdvmqz9RtcBOOYxeSRtIbztMaXfk/FtdWbEA2utj1cw5gCoo8igT6sELujckOcv6+xVZ0iHBtSKCnMNhoF3uOA1Kc24d7D7+/Gd2m++Idw3pA23DWjldeyz2850CXy8Rzx7TEQoUWE2QpSOY37gy5V+0SCdm8TTql4svqQdznFZ1dsysokIDeHZi93hmYt3HOGmj5a6whfrxVm7BDy5dUBL2jXQ/vkx36/llzX7WbM3s8RB3vxCBwNfm0P/l2fz9uytrpTBdkcRq/ZkctNHS13RM2ZOmkJHEdPXWc+8XbDtkJeLKyMr32+Qd+Lc7V6rO5lvFF8s2V3sotS/rtvvN2nKirxCBw9/tZqLJrgfAi2ftM5ZE0hU7Y4iHpqykke+do9nWAn4hNnbuOuz5RiGQY8XZtBp7O9ePnLQs4Tnb8lwZf/0nbk7bc1+l3FStwSXWlkQAa+OJLSGS5yRJBFxcI93HDSFJ7SbpVE3d1l+lvfg5F9vu90qs1+AdGeSqR1zS77/tIfL3PSTwXShhIaEEBlm450be9Iy0TqLomd6XM/Ut09c0IHHzu/A0xedwWe3nul33j/Oa89lyY25tFtjGtbyFktbiCIpUVvA8VFhPDy0HaP6tuDKHk1QShEVZuO/c7bz0+p9/N/sbV7nvnRFV6+3BRNHkUGvJB1mue1gNjERoUT7RJIs2HqIl6dvIq+wyDLW3peGtSKJj/J+ezieZ6dBfPHi/8ECt1tt/B9bXLHqx/MKXYL123o9cch8w9h2MJv0AIt7jPpoqZdf/YqJi9jnMzHptd83uxJ6AV6J0H4uJiVDVp7dZYEXh7kgtinKVknNTEz3je8qTqv3ZvLDqn1MXenOQxRoScAFWw+5UjX44in65nba4Rzu/WKFK2e+J57LIVYUIuDVlWZOMarVTIt163O8j4dGQtJZ7v38LG2Fe7LLY1An0znSXhoXyYnAgz0ViWmB+7oRrDAt5RAFR0+4/zlMcbztrFYMaOs/u7VeXARvXdudmIhQrurZjIk39PA63qyOvnetqDCiwm08d1lnWiToh4hvDhZPwkK1OCx4fDDdm9f2OtYrSb9BpR/PJybCZhkKOHXlXvLtRf5L6VnQqFYk8ZH+r9/14yMtars5ECCN7pETBa4Bur1Hc/lx1d9+LqJXruzCOJ+JXYCfMP22/gDN6kax+MlzXKGbe49Yp3eILqGvvlkvTc7vpMNXw0NDXAOBUWE2nvx+jWvikycZWXpRjaNOi/+Ix9+LYRh8smiX3zlWFrhJoFh8z0yaM5zJ096dt51f1uwndddRr4dLeGhIuaKqAhFachWhSqjfQfvE256r90d8CDsXwNcj9b4tQmc0/Mdm+HqUnm7/bv/A1wszZ2SWQsCtoloqAXMwrWeLOiXUdMcT90qqyxMXdODzxbt58sIOJ/VPERKiuNAnkVe7hnHM2nTQ0mIqLiOs+UBpVjfaL3Svq0dMfW6Bw9LVcdQpVpFhITSvG+03wDXt/gHc8/kKdh/JoW5MuJcb4IxG8Wzcf5zE2LJZdDsyTnDII8LiwSmr/OrUj48kwmLw+lC2vxtkf2YeDWtF8sY1yYyevCzg4iD/sIgr98TMSe91zrntuG9IGwodBrd+ssyVbKxOdBhfLvUPh5y5IZ3bnMnQzN/Rdo/JWQu2HvJbiARgz9EcnggQYx6o3YPGzw3Yl6MnClwDw1D6KKqTRSzw6kyXERDpjHuNquNeBALghHNGWlxDiEmEvcuKv5Zz8eRSWeD5Jx9PWxYGtEnkmYs78qyFpedLqC2En+8bwAejUujRvA7/ubpbwJCyk8GcnNTawp/tywSPOHfPQdX7h7QhMiyEN69J5sIuDTmjoTtD46HsAuKLyc0eGWZj+oPuN6neTvdL5ya1+P6efnx/Tz+UUl6Tp3q2qA3g8jU3rhXJvMcGWV6/h8/bAejoig37/EMJR/V1L+kXHxnqGtT1xUyMZmI+XMxBaV8XTGn8/ACbDvi3KS5Sz/QNDw1xRSUB7Asw6/bfv250bZsRK5s94rP/9Yt19s5//7qJr1L9Hwhl5ZGvV9P7xVmu/crKLiwCHkyER8MV7+ntxPbu8gjvlK6uBSQ8KTRfa0s5u8xqsk8FYwtR3DqgJdHhpXsR7NK0FnEWroTy0KN5Hf56cghXdG9SYt1Ej0EoTzEZ1L4+m164gMu7N2HiDT2JCrd5uWqSLWa5el4nJiKUF6/ozHW9m/Hpbb1ZPfY8fb/YCHo0r+Oq17VpLV65sgudGuuH+t6jOcz6x9n88sBZNK9r7YZKbuZ+u4kOt/H4MP1384NFZMZjw9wLZ8dFhrnGKIq7pidmfd+3iXiL6/iu2Qp4RZa42uzxEPEdoLbCavKUp3vEc+WpisJqLMSXK50LpFQ0IuDBRrdr4bEdMMBjoDHCx3rse6//eS4LvJSmwDc362iX8rJjno5FP7y95LqniJmPDOTXB9xWb6NaUZaTkHypHR3umhhU0pT1C7s0YnS/JMZe0pHIMBuXJTdmdL8kv1dp0wV0w5kteGl4VyJCbZarKSml+Om+AVzTq7krYdimA1m0rhdLnZjwgO1v6bG03obnh3FJV/80B6AfFrEeYhkfGea174nVjFXAr76ZHM1qso/pPrOFKJ65uCNKWcdde6ZqCLf4zh84p+1JJWwD+OHe/tx2ljvvz8lEh7SqF+M3QatpHev7D2iTyP9d152v7ujDi5d3tqxTXkTAg5GYBLB5/LNE+PxDxVisyjP1Tji+38MSL4HNv8Cvj2mXyx/PQIYzeVTGZr1KUGlZ+Zn+/OJq6xmgVUCb+nGuNLsnQ52YMFdoWmmswXGXdnIlCHvr2u6Mu7QTq8eex+qx57mEPLIU1/GlWd1obunfkrev71Fi3UQfsfF0Z3x5ex9qRYXx3d19SX16qFe9uEj/6BmTewa1tiz3FfBrevnngzdpU18bHS8P78KtA1qSEKPb5fscivZItxBhEbEzpEN9mgQQUIChZzTwK6sTHeb1JlfbJ2/Ozf2TAl7v/ZtS+GCUdxhuIAH/5JbeXNKtMWe2Sig2B395EAGvCfgKuOk392XTNP/cKcWhQnRky6IJ8NkIXfbxxXqVoHyfASdHIRzwz2GBw+mKObwNVk8p/b2rIXWiwznPuUpQSRZ4IOIiw/SsT6dlWZooFCuevaQjZ7erV2I934FYz0Hfjo3iWT32PHq2qIsv0eE2YiNCeeTcdky9p5/XMdPa7+JMdja8h3Y/+frMo8NDiQgNsbTY29aPY+2487jKueiHGTvv626KLcGFEhth85oj4MvZ7f2/o9rOcD7Tl1/b543nhjNb+J0DsP6582ldL9bvQdK0trf76sFz2jLvsUGlcq2UlzILuFKqmVJqjlJqo1JqvVLqwYpsmHASNO/nPfknJIBP2SgKPAvTiphEMJyhdOZ55uIS2enaOt+9GHKPwoxndRTM0TTva9g9XosD5C7X1z9xyqJfSsuj57WjXlwETwzr4AoDe/3qbix4fHCpLPDicCfsqjwb6sFz2nJeJ38L1MQ3thzcWRzN3DYPnNOWbk1ru463csbpLxozhCl39CHt5Yt4/epkwPqhtnrsefxkkeo4RCkvK9iMaU9K8J4H4PkW4Pudt0qMoXW9WF66oqtXuaeLo2Mj/zct0yf/24MDWfHMuX4uK6vvBdwPKN/fmWdaZNAuphYJ1vMZKpry/PXYgX8YhnEG0Ae4VylVcjiBUPE0PxOeSIO2evArYAan+a+VTiQ7XqY/7flul4ujAJZ/7PalZ+2HBf+Bj86Hea/C4onOco+Y2TXfwLYZpevDO/10TpdqxH1D2rLsqaHcPag1W/51AaBf45sFGDA8Gbo6s+qVZiJPaZl2/wAWPuHOGfPwue0Is4XQv00CQzr4u9Ws/Oa/PzSQn+7zDkcNCVF8e1dfPhyVwrd3a2u8ce2ogFEqAH89OQTQFn94aAjT7h/gstQBkpt5vyU2rBXlqu+Jpw88wkfAHx/WHqUUzROieXWEFvGoMBtzPSJy2jXwjy4y+x0TEUrdmHB/AY8MKzbsz7ONE2/owU19k/hodArXOl1G5X24nwxljgM3DGM/sN+5naWU2gg0AYpfZVeoPK54T7tJ6rXXucbf8rZMOJGhfzyJbwrHPaba970Pzn0B3ukLecfcVnNBNvzs8ZL18UXuZeFM8QYt7Ou+g07Dde6W0uJruVcke5dDaAQ0rJyBpLLw9vU9+Hb5Xs4qhRuktJj529+/KcVrYO7z27wXtX71yq4cCTCtvX58pOUEoZQkfzdLcTSq5e0X7tykFv+6vDPhthAuS25Cn1be1xvZpwVfLt1N/zYJrNx9lE3OiBRPH7ivhe8ZvWQOZA7t2MDLsveNWrKybXyt5cgwG788cBZvz9nmWgrPM2TVU8DNeQVDOjSgX+tEkpvVtnxYVhYVMpFHKZUEdAf80n0ppe4A7gBo3rz06zEKZSC6LvS4SW/XaaEt6fqdYO6/retf8CpEJ8B3t+r9xt3h/Bed10rQ+ceLG/TMsZixOf0JnSHRytduRsAYhp7S3/Js/6Xl1n6rZ5jG1g/8JnGyfKCtQcaVIr69MFf/RJ+cYJ0skWE2buxj7WstL2ZseyCuLmZwsby8NqJrwLjv6PBQXr6yq+Wxjo3j2fj8MCLDQhjUvj6dx/4OeFvgvkvmebpX+rdJZMHjg0uMKLGa0Tq4Q33emrWVyLAQV86ZMxrFc/fZrV0C7jkO4PsmYBIZZjupNWcrgnILuFIqFvgOeMgwDL9IfMMwJgGTAFJSUqr/Ehc1iav/pz99BXzkD1CvA8Q30jMM1n2vo05CPaymsCjYNhPSLQYmiyPbOZ1/6ST/YzlHtE98x1z44io493no/6B3CgDzYXLRf6BXCRb8wjehRT9o1vvk2lgcky+AfStLJ/aCH+agZFkwxwU8By49Z7He0j+JNvVjeX++Xm7ON0VBINfWK1d2YcaGg8zcmG6ZPya5WW2+u7sfnRrHe1nX5vXDbMrr2pUxJb6slMtZo5QKQ4v354ZhlLCwo1DlhEbCg6uh9WAt3qAt4J6j9XaYh4CbovrLo2W7l1VGw/mvwntnwTHnjLctf+jP/7PIjrjhx+Kvbxgwcyx8eG7J9U6GfStPrr5QKTx4TltClPcCH/XjIxnRs6krhtwqLtyKa3o157re+sHSIED+mJ4t6vgJs2nh+/4JnYroktJSnigUBXwIbDQM4/WKa5JQKYRGwZg93su3mZhpaD0F/FJzXU2Pv94LXtURLn3vc5d5Rr+UhoxN8MsjevvwVv1ZYLEUVUS8dsO82QX+muh/vKRoGsOA9A2wb8XJtc/Ebu0jFk4ND5/bjh0vXWR5zBTW0GIEfN5jg1g0Zohr38w9E0jALe8Tpt8EqrPboDwulP7ASGCtUmqVs+yfhmFYJ+cVqo4ndull2AKt9NNmKLQbBuf9y13WpKd3nZZnw5l36p/8bJ2uFiA8TocRloXsdP+VhEz2r4bZL+rVhH5/EnrfoR80mbuhQSf3+qCBWDxRx6uXlfzjEOqf3dBFkUO3IaZ8i1oIJ89/r+/B1JV/k1RMFkvfgckLOjdk0fZDroUxSoPpQglkcXcNsD7nqaTMFrhhGAsNw1CGYXQ1DCPZ+SPiXR2Jqu0/2ceT8Bi4/iuo6xPGl3yDe3vEZPd2RKweHI1t4L2oRJervc+Pqa+n/RfHnqX+Ze2GaTfLYo8Zny8kwOQLdbhhbibkZbqPPZ+g0+keWAufX6V97dtn+1/3ZKzqyRfo6BWA1I/gv96RHMz+F7zWquwPr5JwFMI7A2BrKcMwQfd73quWC1TXJJrVjeaBc9qWKv2BSUxEKK9fnUzCSSRAC7Mp7h7Umm/v6ut3bMUz5/L1nf7lpxqZiSkE5tL/c2+H+UwXvvtPeGQTFDmnx9+zGK58Xw+O2jz+Saws1MjacO8yQLnX4uz/kD635UDo9wB0uFjvn3m3+7wDznSfr7TwzlleZNezQP+aCFv/gHmv6GXnfMm3cNUc26tztcx9WaflNTm0BX57Qm9PexgyNnqfv/Fn/RnoDaK8HN8H6WvhR4u8NoH47UmY86L1w0s4aZRSPDGsA109JjKZ1I0JrxaDmZIPXAhMiMcfaKiP71Ap/TPwMZ1npbYzfOreJXpw8our3ClvH9uhrxUSqicTnf2403I33HHmcY30uYahr5vknExS5IA9S/x92bv+9N7fvQh2ztfbS9617k/+MR3K+NWNcHQntOivB3QB5r7kXz/M5xV96ww4tNWj/ejrhEbqsMeYRO1eWj8Vut9YvjBI8w2jpORjRQ7tVqrb0v2AKW7Gq1CjEAtcKB2+8dom3a7VIXeerpSWZ3nXiUlwunFi4dzn3HUv83CRxDj9zb6iF2KD22dDt+u8y+e/5r0/63k9IamxM2d3gy7+bU1fD+PbwJ7F2v++/nv46X7rfgHsnKcnJZl8e7MOyfz1Udi/Spcd2qYnPb3rnC4+7xX46T7Y8hsU5sGyD4t3aWz5HZa+r7fnvgJTnG4r8w2jpCia+a/BhGT9JmA+cEubsKw64yjUb0bzx1d1S6o1IuBC8Vz1CfQYVXI9T8Ki4JrP4LYSXuW736hdJPXOcK88ZIVScOnbEN9Ezzbtdbv38dbuaANG/gAXvQ53zIVR0yCusba0QVvepSXCOUC1+B3/Y8s+cG/Pf1V/ZjnXezSt4IzNeqD3l0dg1ReB7/PF1fqB4LDrh8OmaTo230x5YGWBFxVBdoZO0Wu+OUxI1m8qUPySeAc3wX6LlWe2ziid8Oceha0z3fuZe/SMXYDN0+Hv5SVfozSYYwuzX3BntBT8UMbJxsmWg5SUFCM1NfWU3U8IAkyXycliz9erEBXm6beDT6/Q5YEm4Lw3UEe2lIZ2w/SDa+Y4WGIh4IH4x2b4j3OhjQad9YNl0QQIi4HHdwCGFvUz73IPKo9zPiia9HSL38Pr4a1k9/jCiMl6jdSYRJ0SYPoT2k2U2E776n3p/5B+0/GlyAHPO2eYmt/TgXXaZz7jGWjcQ0cZdbs2cB//dznsmKPdYjEJuv0JbeH+VHdfKmISVMZm+K/HBK1Ht2o31cmy6VedVbP9sNLV3/UXNOrq/UZZDVBKLTcMw2/ChFjgQtVSVj9xaAQkDYC2Q6FZn5LrX/p/2tK/1yPqZfBTMGa3nshUr4N74ej2F+r1Rtudr/eb99Mi7EuDLhDiMTX7vYHu7fR1WrwBCk/Asvfhk0t09IrpFij0WBbM03Kd8axbvEG7bt7oCP+qr/3rpo/fU7xjPHKqZB90bzvs+ge8I37Msnf7a/EGPc4w9U4dcTOxL4yrDamTtZto2Ye6jvkQzNrnTpdweKu/1e9pGO6Y6x6fKA32Av8FQKyWDNzwo3UKY0+mXAdfXmN9rDAPvr9Dj2uAHtCePEwPZp+sYWsYp2wxcE9kEFMIfsKj4YpJUK9d4DqNusG9i/X2PzZra7SWMzveJW/pz6O7tE/azMbYor/O8Nj3Xp30a8cc7e4AuOlHHUq5bSb8cJcuy3ZmYoxt4N42+eNp9/bSSXDGpW4B7nkzLPcI0zT97vU7wkGf3HDfjPbev2ISnHEJ/PkWzHtZl63+ApqmQK9bYeKZOhd7ZG0442L3eS8kwCUTsCTDva4k0x7Sk8DsuWALc0+gmvkcdLrcXW/mOPf26ik6cmfQGB2z/9mVujzlFuh6rc6eWdyb13e3wsafvMu2zYQOF+lsl9tm6BDXr515f8pq8e9ZDGu+0t/xXQv1GAno6//6qE7nUByGoV096eshbYEe/7h9DjQpeaGNikJcKIJwMqz5RqchSPLJcX1om7baczP1gO3+1TDlen1s+AfuzIyXvg3TH3dHitTvCLfNgrSFcHA9zHlJL4JxXyokttV+6QWvw4Lxbn94dIIWl7+Xw9Dn9ODlzgXwv8t0xkXTSq7dAjJ3la2fIaE6PLMyiG2oha/lQD3jN30DtOirP3f9qVMkWNFyoJ44tvkXGPi4e/xhbKb/w8BeoB9kZgbNZ4+4B3kNQwvu1hnut6T7UmHDD/oNyeT22TokNlAWy+Ufe2foBDjrUTjnGe+yWc/rh/Xl7xQ/H6MYArlQRMAFobLY8JMW86SztPuiYRf9trD0fe2SCIuC4e9DYhv3Ocf+huN/+yfoOnFYR8Ws/ExnnPS0fn05skNbvUc8JlE16KLjyhPb64Rj5sBjWIx28QCc/2/4c4I+/uwRZzqCtfpabc+HNVP8B1XDYuCa/2nf8cBHddy6Z+QOQO87tcAe2gL7VkGuRU76Oi11/h27z0Bqq8H6TWjOv/zPMbnhO33NWs30g2DrTPj8Su8654yFLlfpuQTzXvEeDwkJgz53aVfKlt/8r9+8H1z1sX7oxDWA1V/Bqs+s8/2A9rnfOR82/aJ/12bWzmu/hA4XBu5HMYiAC8LpREGOdkOsnqIH5c4eo/3qRpEWrN/GwOB/QnxjLdRGkbZQj+/Xk4iaeqRSMN0dRQ5Y+IaOO2/eV79FJA3Q+edNsg7Aov/T1wuN0HHxQ57yzpmz8E34O1VPhmo3TPu7zbw4oF1YZjKzZ4/qe88cp6N5ThyEqLrWDwHQg71mNE5xtB4C3a6HWk21oO+Yo8u7XQ/D/g1fXq/nFgTC88FXGvo/qLNvlhERcEEQqhc5R9x513cugKXv6YHl+mfAnmWwayEMeNhdf8032hU1+GmIrgML3oChY7UrKXWydj1F1YUeI6FRsnaJhMXoDJNK6URu8Y2h/QU6t4/NOQCdsUXn2/l7OYz4SIt71gHYPsc9vgHeD45b/tDunma99ZtOwQk9oWvuy/oNJiJeC3adJO2/H/iYfhsrIyLggiAEP7mZWhytJpYd2aF95LEVtMqRYeixh1aDtdg36qbj/PemumfwWp1jvrFU1IIkBBZwiUIRBCF4KM6KrduqYu9lporwJCIusHib51SgcJeExIELgiAEKSLggiAIQYoIuCAIQpAiAi4IghCkiIALgiAEKSLggiAIQYoIuCAIQpAiAi4IghCknNKZmEqpDKCM6dFIBE59wt3KQfpSPakpfakp/QDpi0kLwzD8ppieUgEvD0qpVKuppMGI9KV6UlP6UlP6AdKXkhAXiiAIQpAiAi4IghCkBJOAT6rqBlQg0pfqSU3pS03pB0hfiiVofOCCIAiCN8FkgQuCIAgeiIALgiAEKUEh4EqpYUqpzUqpbUqpMVXdnpJQSn2klDqolFrnUVZXKTVDKbXV+VnH49iTzr5tVkqdXzWt9kcp1UwpNUcptVEptV4p9aCzPBj7EqmUWqqUWu3sy3PO8qDrC4BSyqaUWqmUmubcD9Z+pCml1iqlVimlUp1lwdqX2kqpb5VSm5z/M30rvS+GYVTrH8AGbAdaAeHAaqBjVberhDYPBHoA6zzKXgXGOLfHAK84tzs6+xQBtHT21VbVfXC2rRHQw7kdB2xxtjcY+6KAWOd2GLAE6BOMfXG27xHgC2BasP59OduXBiT6lAVrXz4BbnNuhwO1K7svwWCB9wa2GYaxwzCMAmAKcFkVt6lYDMOYD/gum30Z+heM8/Nyj/IphmHkG4axE9iG7nOVYxjGfsMwVji3s4CNQBOCsy+GYRjZzt0w549BEPZFKdUUuAj4wKM46PpRDEHXF6VUPNpw+xDAMIwCwzAyqeS+BIOANwH2eOzvdZYFGw0Mw9gPWhiB+s7yoOifUioJ6I62XIOyL063wyrgIDDDMIxg7cubwONAkUdZMPYD9EP0D6XUcqXUHc6yYOxLKyADmOx0bX2glIqhkvsSDAJutUJoTYp9rPb9U0rFAt8BDxmGcby4qhZl1aYvhmE4DMNIBpoCvZVSnYupXi37opS6GDhoGMby0p5iUVbl/fCgv2EYPYALgHuVUgOLqVud+xKKdpu+YxhGd+AE2mUSiArpSzAI+F6gmcd+U2BfFbWlPKQrpRoBOD8POsurdf+UUmFo8f7cMIzvncVB2RcT56vtXGAYwdeX/sClSqk0tDtxiFLqM4KvHwAYhrHP+XkQmIp2IwRjX/YCe51vdQDfogW9UvsSDAK+DGirlGqplAoHrgV+quI2lYWfgFHO7VHAjx7l1yqlIpRSLYG2wNIqaJ8fSimF9ultNAzjdY9DwdiXekqp2s7tKGAosIkg64thGE8ahtHUMIwk9P/CbMMwbiTI+gGglIpRSsWZ28B5wDqCsC+GYRwA9iil2juLzgE2UNl9qeqR21KO7l6IjoDYDjxV1e0pRXu/BPYDhegn7a1AAjAL2Or8rOtR/yln3zYDF1R1+z3aNQD9WrcGWOX8uTBI+9IVWOnsyzrgWWd50PXFo32DcEehBF0/0H7j1c6f9eb/djD2xdm2ZCDV+Tf2A1CnsvsiU+kFQRCClGBwoQiCIAgWiIALgiAEKSLggiAIQYoIuCAIQpAiAi4IghCkiIALgiAEKSLggiAIQcr/AzTYnimcOrgJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a1af18",
   "metadata": {},
   "source": [
    "### Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ac6e611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('modelo_clasificacion.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e957df",
   "metadata": {},
   "source": [
    "## Test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cd53420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.125732\n",
      "\n",
      "Test Accuracy of     1: 95% (92/96)\n",
      "Test Accuracy of     0: 98% (125/127)\n",
      "Test Accuracy of     2: 70% (12/17)\n",
      "\n",
      "Test Accuracy (Overall): 95% (229/240)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(len(classes)))\n",
    "class_total = list(0. for i in range(len(classes)))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    #print(\"-> Correct: \",correct.shape)\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        #print(i,\"Label:{} - Correct: {} \".format(label,correct.shape))\n",
    "        #print(i,\"Correct[i].item(): \",correct[i].item())\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ede795",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e92ae",
   "metadata": {},
   "source": [
    "Dentro de las combinaciones de las configuraciones para los distintos parámetros e hiperparámetros se pudo obtener que con la siguiente configuración se obtuvieron los mejores resultados:\n",
    "    - Learning Rate de 0.05. Se probó con 0.01, 0.05, 0.1 y 1.\n",
    "    - 1 Capa de entrada (35 características), 1 oculta (de 18 a 9 perceptrones, fully connected) y 1 de salida (9 perceptrones a 3 de clases). Se probó aumentando la cantidad de capas ocultas a 3, en total, pero la precisión bajó al 52% dado que se especializaba en la clasificación de la clase 0.\n",
    "    - 400 épocas. inicialmente se probó con 25 épocas, pero a medida que se iba aumentando de 25 en 25, la precisión iba aumentando; logrando una mayor precisión a las 400 épocas, luego de esta cantidad se mantiene la precisión.\n",
    "\n",
    "Para este análisis se contaron con un total de 1200 registros, de los cuales se usaron el 80% para entrenamiento y validación; y el 20 % para pruebas. Para fines teóricos se utilizó una semilla con valor 3 de manera que siempre se obtenga el mismo resultado de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df940a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
